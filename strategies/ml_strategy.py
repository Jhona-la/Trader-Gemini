"""
ML STRATEGY H√çBRIDA DEFINITIVA ULTIMATE
Combina TODO lo mejor de ambas versiones para crecimiento exponencial con riesgo controlado.
Objetivo: Convertir 12 USD en 100K USD en el menor tiempo posible (semanas).
Caracter√≠sticas avanzadas:
- ‚úÖ Ensemble completo de 3 modelos (RF+XGB+GB) con weighted voting din√°mico
- ‚úÖ Detecci√≥n de r√©gimen de mercado avanzada con suavizado temporal
- ‚úÖ Circuit breaker autom√°tico por drawdown y p√©rdidas consecutivas
- ‚úÖ Feature engineering adaptativo por r√©gimen
- ‚úÖ Targets din√°micos por volatilidad y r√©gimen
- ‚úÖ Re-pesado din√°mico de modelos basado en performance
- ‚úÖ Gesti√≥n de riesgo multi-capa con filtros robustos
- ‚úÖ Aprendizaje adaptativo con learning rate variable
- ‚úÖ Monitoreo completo con 40+ m√©tricas
- ‚úÖ Arquitectura as√≠ncrona optimizada para alta frecuencia
"""

# ‚ö†Ô∏è CRITICAL: Suprimir warnings TOTALMENTE para evitar ruido en consola
import os
import warnings

# Disable fragmentation warnings and sklearn parallel noise
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['PYTHONNOUSERSITE'] = '1'
warnings.filterwarnings('ignore')
try:
    from pandas.errors import PerformanceWarning
    warnings.filterwarnings('ignore', category=PerformanceWarning)
except:
    pass
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', message='.*sklearn.utils.parallel.delayed.*')

import numpy as np
import pandas as pd

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier
from strategies.phalanx import OrderFlowAnalyzer, OnlineGARCH  # PHASE 13: Phalanx-Omega Protocol
from strategies.components.feature_engineering import FeatureEngineering # Phase I: Refactoring
from strategies.components.signal_generator import SignalGenerator # Phase I: Refactoring
from strategies.components.models.factory import ModelFactory # Phase I: Refactoring
from core.xai_engine import XAIEngine # Phase 22: Explainability

from .strategy import Strategy
from core.events import SignalEvent
from core.enums import EventType, SignalType
from datetime import datetime, timezone
import talib
from utils.math_helpers import safe_div
from config import Config
from utils.logger import logger
from core.transparent_logger import monitor_log
from utils.debug_tracer import trace_execution
from utils.thread_monitor import monitor
from concurrent.futures import ThreadPoolExecutor
from core.neural_bridge import neural_bridge
import threading
import copy
from collections import deque, Counter
try:
    import ujson as json
except ImportError:
    import json
import joblib
import time
import gc
from data.feature_store import FeatureStore
from core.ml_governance import MLGovernance
from strategies.ml_worker import train_model_process  # WORKER IMPORT
from core.enums import EventType, SignalType, OrderSide, OrderType
import multiprocessing
import asyncio
from utils.shm_utils import SharedMemoryManager # Phase 10
from core.online_learning import OnlineLearner
from ml.replay_buffer import PrioritizedReplayBuffer
from sophia.rewards import reward_engine, TesisDecayReason
from core.reward_system import TradeOutcome
from core.xai_engine import XAIEngine

# Global Process Pool for Training (Singleton)
# Limit to cpu_count - 2 to leave room for Engine and Data Loader
_TRAINING_POOL = None

def get_training_pool():
    global _TRAINING_POOL
    if _TRAINING_POOL is None:
        # CONSERVATIVE SCALING (Ryzen 5700U 1.8GHz Base)
        # Instead of cpu_count - 2, we use simpler logic to prevent thermal throttling:
        # Use 6 workers. Leaves 2 cores for OS/Engine + headroom for heat dissipation.
        # This allows sustaining 1.8GHz-2.5GHz without heavy throttling.
        max_workers = 6 
        
        from concurrent.futures import ProcessPoolExecutor
        _TRAINING_POOL = ProcessPoolExecutor(max_workers=max_workers)
    return _TRAINING_POOL
def ml_inference_worker_task(in_q, out_q):
    """Isolated process task for ML Inference (No GIL contention) (SUPREMO-V3)"""
    # Prevent circular imports if any, but enums are at top
    from core.enums import SignalType
    import time
    
    while True:
        try:
            data = in_q.get()
            X, rf, xgb, gb = data['X'], data['rf'], data['xgb'], data['gb']
            
            # Heavy inference
            rf_p = rf.predict_proba(X)[0][1]
            xgb_p = xgb.predict_proba(X)[0][1]
            gb_p = gb.predict_proba(X)[0][1]
            
            # Weighted ensemble (Dynamic Weights from Main Process)
            # Default to equal weights if not present (Safety)
            w_rf, w_xgb, w_gb = data.get('weights', (0.34, 0.33, 0.33))
            
            conf = (rf_p * w_rf + xgb_p * w_xgb + gb_p * w_gb)
            
            # Signal Logic
            # Note: In worker we only compute raw probability/direction
            # The complex circuit breaking etc stays in the main process
            sig = SignalType.NEUTRAL
            if conf > 0.65: sig = SignalType.LONG
            elif conf < 0.35: sig = SignalType.SHORT
            
            out_q.put({
                'confidence': conf, 
                'signal_type': sig, 
                'rf': rf_p, 'xgb': xgb_p, 'gb': gb_p,
                'ts': data.get('ts', time.time())
            })
            
            # PHASE 3: Neural Insight Publication (Binary SHM)
            try:
                from core.neural_bridge import neural_bridge
                neural_bridge.publish_insight(
                    strategy_id="ML_CORE_WORKER",
                    symbol=data.get('symbol', 'UNKNOWN'),
                    insight={
                        'confidence': conf,
                        'direction': sig.name if hasattr(sig, 'name') else str(sig)
                    }
                )
            except Exception as bridge_err:
                # Silently fail in worker, but we should ideally log to a file
                pass
        except EOFError:
            break # Queue closed
        except Exception:
            time.sleep(0.5)

# MODO PROFESOR: Limitador global de entrenamiento.
# QU√â: Sem√°foro para controlar cu√°ntas estrategias entrenan simult√°neamente.
# POR QU√â: Con 24 s√≠mbolos, n_jobs=-1 causa agotamiento de RAM instant√°neo (97%+).
# PARA QU√â: Estabilizar el sistema y permitir que el trading contin√∫e sin lag.
TRAINING_LIMITER = threading.BoundedSemaphore(value=2) 

class MLStrategyHybridUltimate(Strategy):
    """
    ML Strategy H√≠brida DEFINITIVA ULTIMATE
    Versi√≥n final que combina todas las caracter√≠sticas avanzadas para crecimiento exponencial.
    """
    
    def __init__(self, data_provider, events_queue, symbol='BTC/USDT', 
                 lookback=50, sentiment_loader=None, portfolio=None, risk_manager=None):
        
        # ============================================================
        # ‚úÖ CORE CONFIGURATION - OPTIMIZADO PARA CRECIMIENTO R√ÅPIDO
        # ============================================================
        self.data_provider = data_provider
        self.events_queue = events_queue
        self.symbol = symbol
        initial_lookback = getattr(Config.Strategies, 'ML_LOOKBACK_BARS', 5000)
        self.lookback = min(lookback, initial_lookback)
        self._feature_cols = [] # Ensure initialized
        self.sentiment_loader = sentiment_loader
        self.portfolio = portfolio
        self.risk_manager = risk_manager # Regime Leader Link
        self.strategy_id = "ML_HYBRID_ULTIMATE_V2"
        
        # === ASYNC EXECUTOR OPTIMIZADO ===
        self.executor = ThreadPoolExecutor(
            max_workers=2, 
            thread_name_prefix=f"ML_Ultimate_{symbol.replace('/','')}"
        )
        
        # ============================================================
        # ‚úÖ ENSEMBLE COMPLETO - 3 MODELOS CON WEIGHTED VOTING
        # ============================================================
        self.rf_model = None
        self.xgb_model = None
        self.gb_model = None
        self.scaler = StandardScaler()
        
        # Pesos base para crecimiento agresivo
        self.base_rf_weight = 0.45
        self.base_xgb_weight = 0.35
        self.base_gb_weight = 0.20
        
        # Pesos originales para reset
        self.original_rf_weight = 0.45
        self.original_xgb_weight = 0.35
        self.original_gb_weight = 0.20
        
        # --- SUPREMO-V3: ML ISOLATION ---
        self._inference_queue = multiprocessing.Queue(maxsize=10)
        self._results_queue = multiprocessing.Queue(maxsize=10)
        self._inference_process = None
        
        # ============================================================
        # ‚úÖ PERFORMANCE TRACKING PARA CRECIMIENTO EXPONENCIAL
        # ============================================================
        self.performance_history = deque(maxlen=100)  # Historial de trades
        self.performance_window = deque(maxlen=20)    # Ventana para c√°lculos din√°micos
        self.signal_history = deque(maxlen=100)       # Historial completo de se√±ales
        self.equity_curve = deque(maxlen=500)         # Curva de equity para an√°lisis
        
        # Tracking individual por modelo
        self.individual_model_scores = {'rf': 0.0, 'xgb': 0.0, 'gb': 0.0}
        self.model_performance = {
            'rf': deque(maxlen=30),
            'xgb': deque(maxlen=30),
            'gb': deque(maxlen=30)
        }
        
        # ============================================================
        # ‚úÖ TRAINING CONFIGURATION - OPTIMIZADO PARA RAPIDEZ
        # ============================================================
        self.is_trained = False
        self.min_bars_to_train = 300  # Reducido para empezar r√°pido
        self.bars_since_train = 0
        self.retrain_interval = 150   # Retrain m√°s frecuente
        self.last_training_time = None
        self.last_training_score = 0.0
        self.training_iteration = 0
        
        # ============================================================
        # TARGETS ADAPTATIVOS - ULTRA REDUCIDOS PARA TESTNET/BAJA VOL
        # ============================================================
        self.BASE_TP_TARGET = 0.003   # 0.3%
        self.BASE_SL_TARGET = 0.002   # 0.2%
        # Aumentado para microscalping M1/M5: 30 velas = 30 min context
        self.LOOKAHEAD_BARS = 30
        
        self.current_tp_target = self.BASE_TP_TARGET
        self.current_sl_target = self.BASE_SL_TARGET
        self.volatility_multiplier = 1.0
        
        # ============================================================
        # ‚úÖ UMBRALES ADAPTATIVOS - BALANCE ENTRE RIESGO Y GANANCIA
        # ============================================================
        self.MIN_MODEL_ACCURACY = 0.35  # Aggressive: 35% accuracy floor (ML > Random)
        
        # Umbrales base optimizados para SMART GROWTH MODE
        self.BASE_CONFIDENCE_THRESHOLD = 0.55  # Winning Probability > 55%
        self.BASE_CONFLUENCE_LONG = 0.25       # More permissive confluence
        self.BASE_CONFLUENCE_SHORT = -0.30     # M√°s permisivo
        
        # Umbrales adaptativos
        self.adaptive_confidence_threshold = self.BASE_CONFIDENCE_THRESHOLD
        
        # OMEGA MIND: Online Learner for weights
        self.online_learner = OnlineLearner(learning_rate=0.005) 
        self.last_ensemble_input = None # Store [rf, xgb, gb] for update loop
        
        # Phase 9: Neural Fortress Components
        self.memory = PrioritizedReplayBuffer(capacity=2000)
        self.reward_system = RewardSystem()
        self.training_batch_size = 32
        self.steps_since_learn = 0
        self.xai_engine = XAIEngine()
        self.last_hmm_info = {'regime': 'UNKNOWN', 'transition_risk': 0.0}
    
    def __hash__(self):
        """Force hashable for ensemble orchestration."""
        return id(self)
        
    def __eq__(self, other):
        """Standard equality for list removal."""
        return self is other
        self.adaptive_confluence_short = self.BASE_CONFLUENCE_SHORT
        
        # ============================================================
        # ‚úÖ PHASE 13: PHALANX-OMEGA COMPONENTS
        # ============================================================
        self.phalanx = OrderFlowAnalyzer()
        # Crypto Params: Alpha=0.05 (Reaction), Beta=0.9 (Persistence)
        self.garch = OnlineGARCH(omega=1e-6, alpha=0.05, beta=0.90, initial_variance=1e-4) 

        
        # ============================================================
        # ‚úÖ FILTROS DE ROBUSTEZ - PROTECCI√ìN EN CRECIMIENTO R√ÅPIDO
        # ============================================================
        self.MAX_ATR_PCT = 0.035      # M√°s permisivo para volatilidad
        self.MIN_VOLUME_RATIO = 0.7   # Menos restrictivo
        self.RSI_FILTER_RANGE = (20, 80)  # Rango m√°s amplio
        # === INFRAESTRUCTURA DE PERSISTENCIA Y ENTRENAMIENTO INCREMENTAL ===
        self._state_lock = threading.Lock()
        self.loop_count = 0
        self.analysis_stats = Counter()
        self.bars_since_incremental = 0  # Contador para updates r√°pidos
        self.models_dir = ".models"
        os.makedirs(self.models_dir, exist_ok=True)
        
        # === [PHASE 12] GOVERNANCE & FEATURE STORE ===
        self.feature_store = FeatureStore()
        self.ml_governance = MLGovernance()
        
        # Cargar modelos previos si existen (Prioridad: MLGovernance)
        # SUPREME BLOCK V: Bypass Governance to force new XGB JSON models
        # self._load_governed_model()
        self._load_models()
        
        # ============================================================
        # ‚úÖ DETECCI√ìN DE R√âGIMEN DE MERCADO AVANZADA
        # ============================================================
        # DETECCI√ìN DE R√âGIMEN DE MERCADO AVANZADA
        self.market_regime = "UNKNOWN"
        self.regime_history = deque(maxlen=15)
        self.last_regime_update = datetime.now(timezone.utc) - pd.Timedelta(minutes=5) # Allow immediate update
        self.regime_confidence = 0.0
        self.regime_duration = 0
        self.oracle_log_count = 0  # Counter for throttling repetitive logs
        
        # ============================================================
        # ‚úÖ CIRCUIT BREAKER AVANZADO - PROTECCI√ìN CAPITAL
        # ============================================================
        self.circuit_breaker_active = False
        self.circuit_breaker_threshold = 0.12  # 12% drawdown para crecimiento agresivo
        self.original_confidence_threshold = self.BASE_CONFIDENCE_THRESHOLD
        self.peak_equity = None
        self.consecutive_losses = 0
        self.max_consecutive_losses = 4        # M√°s sensible a p√©rdidas
        
        # ============================================================
        # ‚úÖ SISTEMA DE APRENDIZAJE ADAPTATIVO
        # ============================================================
        self.learning_rate = 1.0
        self.aggressiveness_factor = 1.0  # Factor de agresividad din√°mico
        self.win_streak = 0
        self.loss_streak = 0
        
        # ============================================================
        # ‚úÖ STATE MANAGEMENT Y THREAD SAFETY
        # ============================================================
        self.running = True
        self._state_lock = threading.Lock()
        self._feature_cols = None
        self._training_thread = None
        self._last_prediction_time = None
        self._label_mapping = {0: -1, 1: 1}  # Default mapping for inference before training
        
        # ============================================================
        # ‚úÖ MONITORING Y ESTAD√çSTICAS COMPLETAS
        # ============================================================
        self.total_signals_generated = 0
        self.total_trades = 0
        self.winning_trades = 0
        self.losing_trades = 0
        self.max_win_streak = 0
        self.max_loss_streak = 0
        
        # Win Rate / Payoff tracking for Kelly
        self.rolling_win_rate = deque(maxlen=20)
        self.rolling_payoff = deque(maxlen=20) # Avg Win / Avg Loss
        
        self.signals_by_regime = {
            'TRENDING': 0, 'RANGING': 0, 'VOLATILE': 0, 'MIXED': 0, 'UNKNOWN': 0
        }
        
        self.regime_accuracy = {
            'TRENDING': [], 'RANGING': [], 'VOLATILE': [], 'MIXED': [], 'UNKNOWN': []
        }
        
        # ============================================================
        # ‚úÖ CONFIGURACI√ìN DE CRECIMIENTO EXPONENCIAL
        # ============================================================
        self.compounding_factor = 1.0  # Factor de compuesto
        self.position_sizing_mode = "KELLY"  # Kelly, FIXED, VOLATILITY
        self.kelly_fraction = 0.5  # Fracci√≥n de Kelly para riesgo controlado
        self.base_position_size = 0.95  # Usar 95% del capital para crecimiento r√°pido
        
        # Phase 5: Dynamic Math Utils
        # from utils.statistics_pro import StatisticsPro (Deprecated in Phase 13 for Inline Kelly)
        # self.stats_pro = StatisticsPro()

        
        # Meta de 12 USD a 100K USD
        self.initial_capital = 12.0
        self.target_capital = 100000.0
        self.current_capital = self.initial_capital
        
        logger.info(f"üü¢ ML HYBRID ULTIMATE STRATEGY [ENSEMBLE] INITIALIZED FOR {symbol}")
        logger.info(f"üéØ OBJECTIVE: ${self.initial_capital} ‚Üí ${self.target_capital}")
        logger.info(f"‚öôÔ∏è  Mode: Exponential Growth (Aggressive with Risk Control)")
        
    def _calculate_dynamic_sizing(self, confidence, volatility):
        """
        Phase 5: Dynamic Kelly Criterion Sizing
        Size = Kelly% * Confidence_Scaler * Volatility_Scaler
        """
        try:
            # 1. READ REAL STATS FROM PORTFOLIO (PHALANX-OMEGA)
            kelly_pct = 0.05 # Default Safe Fallback
            
            if self.portfolio and self.strategy_id in self.portfolio.strategy_performance:
                perf = self.portfolio.strategy_performance[self.strategy_id]
                trades = perf['trades']
                
                if trades >= 10:
                    wins = perf['wins']
                    losses = perf['losses']
                    total_win_pnl = perf.get('total_win_pnl', 0.0) # Safety get for old state
                    total_loss_pnl = perf.get('total_loss_pnl', 0.0)
                    
                    if losses > 0 and total_loss_pnl > 0:
                        avg_win = total_win_pnl / wins if wins > 0 else 0
                        avg_loss = total_loss_pnl / losses
                        
                        # Kelly Variables
                        p = wins / trades       # Win Probability
                        q = 1.0 - p             # Loss Probability
                        b = avg_win / avg_loss  # Payoff Ratio
                        
                        # Full Kelly Formula: f = p - q/b
                        if b > 0:
                            raw_kelly = p - (q / b)
                            kelly_pct = max(0.01, raw_kelly) # Floor at 1%
                            
                            # Log periodically
                            if trades % 5 == 0:
                                logger.info(f"üß† [KELLY] p={p:.2f} b={b:.2f} => Raw K={raw_kelly:.2f}")

            # 2. Apply Fractional Kelly (Safety)
            # We use 'kelly_fraction' (e.g. 0.3 or 0.5) to be conservative
            safe_kelly = kelly_pct * getattr(self, 'kelly_fraction', 0.4)
            
            # 3. Scale by Prediction Confidence (Higher conf -> Higher size)
            # Normalize confidence (0.5 to 1.0) -> (0.0 to 1.0)
            conf_scaler = max(0.0, (confidence - 0.5) * 2) 
            
            # 4. Scale by Volatility (Inverse Volatility Sizing)
            # If Volatility is high, reduce size to keep $ Risk constant
            # Reference volatility: 0.005 (0.5%) per bars
            vol_scaler = min(1.5, 0.005 / max(0.001, volatility))
            
            final_size = min(0.95, safe_kelly * conf_scaler * vol_scaler)
            
            # print(f"  üí∞ Dynamic Sizing: K={kelly_pct:.2f} -> Safe={safe_kelly:.2f} * Conf={conf_scaler:.2f} * Vol={vol_scaler:.2f} = {final_size:.2f}")
            return max(0.1, final_size) # Min 10%

            
        except Exception as e:
            logger.error(f"Sizing error: {e}")
            return 0.1

    # ============================================================
    # ‚úÖ DETECCI√ìN DE R√âGIMEN MEJORADA CON SUAVIZADO
    # ============================================================
    
    def _detect_market_regime(self, df):
        """
        Detecci√≥n avanzada de r√©gimen con m√∫ltiples capas de validaci√≥n
        """
        if len(df) < 50:
            return "UNKNOWN", 0.0
            
        try:
            # Indicadores principales
            current_adx = df['adx'].iloc[-1] if 'adx' in df.columns else 20
            current_atr_pct = (df['atr_pct'].iloc[-1] / 100) if 'atr_pct' in df.columns else 0.01
            rsi_std = df['rsi_14'].tail(20).std() if 'rsi_14' in df.columns else 15
            
            # Volatilidad y tendencia
            price_volatility = df['close'].pct_change().tail(20).std()
            volume_volatility = df['volume'].pct_change().tail(20).std()
            
            # Tendencia EMAs
            closes = df['close'].values
            ema_20 = talib.EMA(closes, timeperiod=20)[-1] if len(closes) >= 20 else closes[-1]
            ema_50 = talib.EMA(closes, timeperiod=50)[-1] if len(closes) >= 50 else closes[-1]
            trend_strength = abs(ema_20 - ema_50) / ema_50 if ema_50 > 0 else 0
            
            # Sistema de scoring mejorado
            regime_scores = {
                'TRENDING': 0.0,
                'RANGING': 0.0,
                'VOLATILE': 0.0,
                'STAGNANT': 0.0,
                'MIXED': 0.0
            }
            
            # ‚úÖ TRENDING: ADX alto + tendencia fuerte + volatilidad controlada
            if current_adx > 25:
                regime_scores['TRENDING'] += 0.4
            if trend_strength > 0.025:
                regime_scores['TRENDING'] += 0.3
            if current_atr_pct < 0.03:
                regime_scores['TRENDING'] += 0.2
            if volume_volatility < 0.5:
                regime_scores['TRENDING'] += 0.1
                
            # ‚úÖ VOLATILE: ATR alto + RSI vol√°til + alta volatilidad precio
            if current_atr_pct > 0.035:
                regime_scores['VOLATILE'] += 0.5
            if rsi_std > 18:
                regime_scores['VOLATILE'] += 0.3
            if price_volatility > 0.035:
                regime_scores['VOLATILE'] += 0.2
                
            # ‚úÖ RANGING: ADX bajo + RSI estable + baja volatilidad
            if current_adx < 20: # Un poco m√°s permisivo que 18
                regime_scores['RANGING'] += 0.3
            if rsi_std < 10: # Un poco m√°s permisivo que 8
                regime_scores['RANGING'] += 0.3
            if current_atr_pct < 0.015: # < 1.5%
                regime_scores['RANGING'] += 0.2
            
            # ‚úÖ STAGNANT (ZOMBIE): Volatilidad nula o insignificante
            # Usar la auditor√≠a de calidad de datos aqu√≠ tambi√©n
            price_spread = (df['high'].max() - df['low'].min()) / df['close'].mean()
            identical_bars = (df['high'] == df['low']).sum() / len(df)
            
            if current_atr_pct < 0.0005 or price_spread < 0.0002 or identical_bars > 0.85:
                regime_scores['STAGNANT'] += 0.8
            elif current_atr_pct < 0.0015: # < 0.15%
                regime_scores['STAGNANT'] += 0.5
                regime_scores['RANGING'] += 0.1
                
            # ‚úÖ MIXED: Sin se√±ales claras o transici√≥n
            if max(regime_scores.values()) < 0.45:
                regime_scores['MIXED'] = 1.0
                
            # Determinar r√©gimen dominante
            best_regime_pair = max(regime_scores.items(), key=lambda x: x[1])
            best_regime = best_regime_pair[0]
            confidence = min(best_regime_pair[1] * 1.2, 1.0)  # Boost de confianza
            
            # --- M√âTRICAS ESTAD√çSTICAS PARA LOGGING ---
            stats = {
                'adx': float(current_adx),
                'atr_pct': float(current_atr_pct) * 100,
                'rsi_std': float(rsi_std),
                'trend_strength': float(trend_strength) * 100
            }
            
            return best_regime, confidence, stats
            
        except Exception as e:
            logger.error(f"Error detecting regime: {e}")
            return "UNKNOWN", 0.0, {}
    
    def _update_market_regime(self, df):
        """
        Actualizar r√©gimen con suavizado y persistencia
        """
        current_time = datetime.now(timezone.utc)
        
        # Solo actualizar cada 3 minutos para evitar overfitting
        # EXCEPCI√ìN: Si es UNKNOWN, actualizar siempre para inicializar
        if self.market_regime != "UNKNOWN" and (current_time - self.last_regime_update).total_seconds() < 180:
            return
            
        new_regime, confidence, stats = self._detect_market_regime(df)
        self.regime_history.append(new_regime)
        
        # Suavizado con ventana din√°mica
        if len(self.regime_history) >= 5:
            regime_counts = Counter(self.regime_history)
            most_common = regime_counts.most_common(2)
            
            if len(most_common) >= 2:
                # Si hay empate cercano, usar MIXED
                if most_common[0][1] - most_common[1][1] <= 2:
                    smoothed_regime = "MIXED"
                else:
                    smoothed_regime = most_common[0][0]
            else:
                smoothed_regime = most_common[0][0]
        else:
            # Si hay poca historia, usamos el √∫ltimo detectado
            smoothed_regime = new_regime
            
        # Solo cambiar si hay confianza suficiente o es consistente o es el primer update real
        is_initial = (self.market_regime == "UNKNOWN" and len(self.regime_history) >= 1)
        if confidence > 0.55 or smoothed_regime == self.market_regime or is_initial:
                with self._state_lock:
                    old_regime = self.market_regime
                    self.market_regime = smoothed_regime
                    self.regime_confidence = confidence
                    self.last_regime_update = current_time
                    
                    # Actualizar duraci√≥n del r√©gimen
                    if old_regime == smoothed_regime:
                        self.regime_duration += 1
                    else:
                        self.regime_duration = 1
                
                # Ajustar todos los par√°metros al nuevo r√©gimen
                self._adjust_all_parameters_by_regime(smoothed_regime)
                
                if old_regime != smoothed_regime:
                    # ORCHESTRATION (Phase 12): Push Global Regime if this is the Leader
                    if self.risk_manager:
                        self.risk_manager.update_regime(smoothed_regime)
                        # Phase 14: Transfer HMM Info to Strategy
                        if hasattr(self.risk_manager, 'transition_risk'):
                            self.last_hmm_info['transition_risk'] = self.risk_manager.transition_risk

                    # --- NARRATIVA CONCEPTUAL Y ESTAD√çSTICA ---
                    descriptions = {
                        "TRENDING": "Directional bias confirmed. Momentum engines active.",
                        "RANGING": "Side-ways consolidation. Switching to Mean Reversion mode.",
                        "VOLATILE": "High noise & volatility. Wide stops and conservative bias.",
                        "STAGNANT": "Zombie market detected. Stagnant price action. Protection active.",
                        "MIXED": "Internal transition or choppy price action. Filtering active.",
                        "UNKNOWN": "Initializing discovery mode."
                    }
                    concept = descriptions.get(smoothed_regime, "Evolving market state.")
                    
                    emoji = "üöÄ" if smoothed_regime == "TRENDING" else "‚öñÔ∏è" if smoothed_regime == "RANGING" else "üî•" if smoothed_regime == "VOLATILE" else "üßä" if smoothed_regime == "STAGNANT" else "üîÑ"
                    
                    logger.info(
                        f"\n{'='*70}\n"
                        f"üìä {emoji} REGIME CHANGE: {self.symbol}\n"
                        f"{'='*70}\n"
                        f"   Phase: {old_regime} ‚Üí {smoothed_regime}\n"
                        f"   Concept: {concept}\n"
                        f"   Stats: ADX={stats.get('adx', 0):.1f} | ATR%={stats.get('atr_pct', 0):.2f}% | Trend={stats.get('trend_strength', 0):.2f}%\n"
                        f"   Confidence: {confidence*100:.1f}% | Strategy: Adaptive targets enabled.\n"
                        f"{'='*70}\n"
                    )

    def _adjust_all_parameters_by_regime(self, regime):
        """
        Ajustar TODOS los par√°metros seg√∫n el r√©gimen de mercado (DYNAMIC & CONFIG DRIVEN)
        """
        # 1. Get Advice from Intelligence Layer (Source of Truth)
        if hasattr(self.market_regime, 'get_regime_advice'):
             advice = self.market_regime.get_regime_advice(regime)
        else:
             # Fallback if market_regime is string/mock
             from core.market_regime import MarketRegimeDetector
             advice = MarketRegimeDetector().get_regime_advice(regime)
        
        # 2. Extract Dynamic Parameters
        lev_limit = advice.get('leverage', 1)
        threshold_mod = advice.get('threshold_mod', 0.0)
        scale_factor = advice.get('scale', 0.0)
        
        # 3. Apply to Strategy State
        self.adaptive_confidence_threshold = max(0.40, min(0.90, self.BASE_CONFIDENCE_THRESHOLD + threshold_mod))
        
        # Scale Targets based on Volatility/Agression
        # Higher Leverage (Sniper) -> Tighter Stops, Bigger Targets (Risk Reward)
        # Lower Leverage (Safety) -> Wider Stops (Volatility Room)
        
        if lev_limit >= 5: # SNIPER / BULL
            self.aggressiveness_factor = 1.2
            self.current_tp_target = self.BASE_TP_TARGET * 1.3
            self.current_sl_target = self.BASE_SL_TARGET * 0.9 # Tight Stop
            self.adaptive_confluence_long = self.BASE_CONFLUENCE_LONG - 0.10
        elif lev_limit <= 1: # DEFENSE / BEAR
            self.aggressiveness_factor = 0.5
            self.current_tp_target = self.BASE_TP_TARGET * 1.0
            self.current_sl_target = self.BASE_SL_TARGET * 1.5 # Wide Stop
            self.adaptive_confluence_long = self.BASE_CONFLUENCE_LONG + 0.20
        else: # NORMAL / RANGING
            self.aggressiveness_factor = 1.0
            self.current_tp_target = self.BASE_TP_TARGET
            self.current_sl_target = self.BASE_SL_TARGET
            self.adaptive_confluence_long = self.BASE_CONFLUENCE_LONG

        # --- PHASE 14: HMM TRANSITION RISK ADJUSTMENT ---
        # Si el riesgo de transici√≥n es elevado (>40%), somos m√°s conservadores.
        trans_risk = self.last_hmm_info.get('transition_risk', 0.0)
        if trans_risk > 0.40:
            self.adaptive_confidence_threshold += 0.05
            self.aggressiveness_factor *= 0.8
            logger.info(f"üõ°Ô∏è [HMM Safety] Transition Risk Elevated ({trans_risk:.2f}). Confidence threshold increased (+5%).")

        # Aplicar learning rate y factor de agresividad
        # NOTE: Threshold is already modulated by Config loop above
        
        logger.debug(
            f"üîß [Dynamic Adaptation] {regime}: "
            f"Lev={lev_limit}x, "
            f"Conf={self.adaptive_confidence_threshold:.2f} (Base {self.BASE_CONFIDENCE_THRESHOLD} + {threshold_mod}), "
            f"Aggr={self.aggressiveness_factor:.1f}"
        )

    # ============================================================
    # ‚úÖ FEATURE ENGINEERING ULTIMATE - 80+ FEATURES
    # ============================================================
    
    @trace_execution
    def _prepare_features(self, bars, regime_aware=True):
        """Delegated to FeatureEngineering component"""
        return self.feature_engineer.prepare_features(
            bars, 
            market_regime=self.market_regime if regime_aware else "UNKNOWN",
            sentiment_loader=self.sentiment_loader,
            data_provider=self.data_provider,
            symbol=self.symbol,
            feature_store=self.feature_store
        )

    def _validate_features(self, df):
        """Delegated to component"""
        return self.feature_engineer.validate_features(df)

    # ============================================================
    # ‚úÖ LABEL CREATION CON TARGETS ADAPTATIVOS
    # ============================================================
    
    def _create_labels(self, df, adaptive_targets=True):
        """
        Creaci√≥n de labels con targets adaptativos por volatilidad
        """
        # CRITICAL PERFORMANCE: Ensure dataframe is consolidated before looping or adding labels
        df = df.copy()
        labels = []
        
        for i in range(len(df)):
            if i >= len(df) - self.LOOKAHEAD_BARS:
                labels.append(0)
                continue
                
            current_price = df.iloc[i]['close']
            current_volatility = df.iloc[i]['atr_pct'] / 100
            
            # Targets adaptativos por volatilidad
            if adaptive_targets:
                # Multiplicadores para targets (H√≠brido Testnet/Producci√≥n)
                if current_volatility > 0.04:  # Muy alta volatilidad
                    volatility_multiplier = 1.5
                elif current_volatility > 0.03:  # Alta volatilidad
                    volatility_multiplier = 1.3
                elif current_volatility > 0.02:  # Volatilidad media
                    volatility_multiplier = 1.1
                elif current_volatility < 0.0003:  # Testnet Extrema (< 0.03%)
                    volatility_multiplier = 0.10 # Target ~0.03% (Ultra-sensitive)
                elif current_volatility < 0.0008:  # Micro Testnet (< 0.08%)
                    volatility_multiplier = 0.20 
                elif current_volatility < 0.002:  # < 0.2%
                    volatility_multiplier = 0.35
                elif current_volatility < 0.01:   # < 1%
                    volatility_multiplier = 0.6
                else:
                    volatility_multiplier = 1.0
                    
                tp_target = self.BASE_TP_TARGET * volatility_multiplier
                sl_target = self.BASE_SL_TARGET * volatility_multiplier
            else:
                tp_target = self.BASE_TP_TARGET
                sl_target = self.BASE_SL_TARGET
                
            future_data = df.iloc[i+1:i+1+self.LOOKAHEAD_BARS]
            
            if len(future_data) == 0:
                labels.append(0)
                continue
            
            tp_level = current_price * (1 + tp_target)
            sl_level = current_price * (1 - sl_target)
            
            tp_hit = (future_data['high'] >= tp_level).any()
            sl_hit = (future_data['low'] <= sl_level).any()
            
            if tp_hit and sl_hit:
                # Ambos tocados: usar el primero
                tp_idx = future_data[future_data['high'] >= tp_level].index[0]
                sl_idx = future_data[future_data['low'] <= sl_level].index[0]
                label = 1 if tp_idx < sl_idx else -1
            elif tp_hit:
                label = 1
            elif sl_hit:
                label = -1
            else:
                # --- FALLBACK: Fixed Horizon Return ---
                # Si no tocamos TP ni SL, usamos el signo del movimiento final
                final_price = future_data.iloc[-1]['close']
                ret = (final_price - current_price) / current_price
                
                # Umbral de ruido equilibrado (50% del target)
                noise_threshold = tp_target * 0.5
                
                if ret > noise_threshold:
                    label = 1
                elif ret < -noise_threshold:
                    label = -1
                else:
                    label = 0
                
            labels.append(label)
        
        # Asegurar longitud correcta
        while len(labels) < len(df):
            labels.append(0)
            
        # DEFRAGMENTATION: Copy before adding new column to large DF
        df = df.copy()
        df['label'] = labels[:len(df)]
        return df

    # ============================================================
    # ‚úÖ TRAINING ULTIMATE - ENSEMBLE CON HIPERPAR√ÅMETROS ADAPTATIVOS
    # ============================================================
    
    def _train_with_cross_validation(self, df):
        """
        Training con ensemble de 3 modelos y hiperpar√°metros adaptativos por r√©gimen
        """
        if len(df) < 200:
            return None, 0.0
            
        # Actualizar r√©gimen antes de entrenar
        self._update_market_regime(df)
        
        # ‚ö° EFICIENCIA DE ENTRENAMIENTO (Rule 3.1)
        # SUBSAMPLING ESTRAT√âGICO
        # PROFESSOR METHOD:
        # QU√â: Selecci√≥n de un subconjunto relevante de datos.
        # POR QU√â: Reduce tiempo de c√≥mputo en 50% y prioriza el r√©gimen de mercado actual.
        # C√ìMO: Si hay > 3000 velas, nos quedamos con el √∫ltimo 60% (relevancia temporal).
        original_len = len(df)
        if original_len > 3000:
            df = df.iloc[-int(original_len * 0.6):]
            logger.info(f"‚ö° [ML Optimization] Subsampling active: {original_len} -> {len(df)} samples (60% most recent).")

        # Crear labels con targets adaptativos
        df = self._create_labels(df, adaptive_targets=True)
        df = df.dropna()
        
        # DEBUG: Analizar por qu√© no hay se√±ales
        vol_mean = df['atr_pct'].mean()
        vol_ref = vol_mean / 100
        mult = 1.0
        if vol_ref < 0.0003: mult = 0.10
        elif vol_ref < 0.0008: mult = 0.20
        elif vol_ref < 0.002: mult = 0.35
        elif vol_ref < 0.01: mult = 0.6
        
        tp_mean = self.BASE_TP_TARGET * mult
        label_counts = df['label'].value_counts().to_dict()
        logger.info(f"üîç DEBUG ML [{self.symbol}]: Rows={len(df)}, AvgVol={vol_mean:.4f}%, mult={mult:.2f}, Est.Target={tp_mean:.4f}, Labels={label_counts}")
        
        df_signals = df[df['label'] != 0]
        
        if len(df_signals) < 30:
            # --- NEW: Data Quality Audit (Transparencia) ---
            price_spread = (df['high'].max() - df['low'].min()) / df['close'].mean()
            identical_bars = (df['high'] == df['low']).sum() / len(df)
            
            if price_spread < 0.0001 or identical_bars > 0.95:
                # Caso Testnet: Precio fijo (ej: BTC=5.0)
                logger.warning(f"üö´ [ZOMBIE MARKET] {self.symbol} is flat. Spread: {price_spread*100:.6f}% | Identical Bars: {identical_bars*100:.1f}%. Training aborted.")
            else:
                logger.warning(f"‚ö†Ô∏è [LOW VOLATILITY] {self.symbol} has movement but not enough to reach targets. Real Signals: {len(df_signals)}/30 required.")
            
            return None, 0.0    # Excluir columnas no-features
        exclude_cols = ['label', 'open', 'high', 'low', 'close', 'volume', 
                       'timestamp', 'datetime', 'symbol']
        feature_cols = [c for c in df_signals.columns if c not in exclude_cols]
        
        X = df_signals[feature_cols]
        y = df_signals['label']
        
        # CRITICAL: Remap labels for XGBoost compatibility
        # XGBoost expects classes starting from 0: [0, 1, 2, ...]
        # Our labels are [-1, 0, 1] -> remap to [0, 1, 2]
        # Note: We filter df_signals where label != 0, so we only have [-1, 1]
        # Remap: -1 -> 0, 1 -> 1
        y = y.map({-1: 0, 1: 1})
        self._label_mapping = {0: -1, 1: 1}  # For inverse mapping during inference
        
        # DEBUG: Verificar si las features son v√°lidas
        std_zero_cols = X.columns[X.std() == 0].tolist()
        if len(std_zero_cols) > 0:
            logger.debug(f"‚ö†Ô∏è {len(std_zero_cols)} features are constant (std=0). Ex: {std_zero_cols[:5]}")
            
        logger.info(f"üìä Training {self.symbol} with {len(X)} samples, {len(feature_cols)} features")
        
        # TimeSeriesSplit CV with auto-adjustment for small datasets
        n_samples = len(X)
        if n_samples < 5:
            # Life Support Mode: Not enough data for CV
            logger.info("üìâ Minimal data mode: Using full dataset for training (Overfitting intended for survival)")
            # Manual split: Train and test on same data to produce a valid score and model
            indices = list(range(n_samples))
            splitter = [(indices, indices)]
        else:
            tscv = TimeSeriesSplit(n_splits=3)
            splitter = tscv.split(X)
            
        cv_scores = {'rf': [], 'xgb': [], 'gb': []}
        best_models = {'rf': None, 'xgb': None, 'gb': None}
        best_scaler = None
        
        # Hiperpar√°metros por r√©gimen y agresividad
        if self.market_regime == "VOLATILE":
            # Conservador en volatilidad
            n_estimators = 70
            max_depth_rf = 5
            max_depth_xgb = 4
            max_depth_gb = 4
            min_samples_split = 10
            learning_rate = 0.05
            subsample = 0.7
            
        elif self.market_regime == "TRENDING":
            # Agresivo en trending
            n_estimators = 120
            max_depth_rf = 8
            max_depth_xgb = 7
            max_depth_gb = 6
            min_samples_split = 5
            learning_rate = 0.08
            subsample = 0.9
            
        elif self.market_regime == "RANGING":
            # Equilibrado en ranging
            n_estimators = 90
            max_depth_rf = 6
            max_depth_xgb = 5
            max_depth_gb = 5
            min_samples_split = 8
            learning_rate = 0.06
            subsample = 0.8
            
        else:  # MIXED o UNKNOWN
            n_estimators = 80
            max_depth_rf = 6
            max_depth_xgb = 5
            max_depth_gb = 5
            min_samples_split = 8
            learning_rate = 0.06
            subsample = 0.8
        
        # Ajustar por agresividad y fase inicial
        # PRIORIDAD:        # Phase I: Model Injection (Factory Pattern)
        models = ModelFactory.get_ensemble_models()
        self.rf_model = models['rf']
        self.xgb_model = models['xgb']
        self.gb_model = models['gb']

        # Pesos del Ensemble (Gobernanza Din√°mica)ento con reporte de progreso
        num_folds = 3 if n_samples >= 300 else 1
        for fold, (train_idx, test_idx) in enumerate(splitter):
            if fold % 1 == 0: # Log every fold
                logger.info(f"   ‚öôÔ∏è [{self.symbol}] Fitting ML Engine (Fold {fold+1}/{num_folds})...")
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # Scaling & Memory Optimization (Rule 3.6)
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train).astype('float32')
            X_test_scaled = scaler.transform(X_test).astype('float32')
            
            # 1. Random Forest (Paralelizaci√≥n completa)
            rf = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth_rf,
                min_samples_split=min_samples_split,
                min_samples_leaf=3,
                max_features='sqrt',
                n_jobs=1,   # Optimizaci√≥n: n_jobs=1 para evitar explosi√≥n de RAM con 24 s√≠mbolos
                random_state=42 + self.training_iteration,
                class_weight='balanced'
            )
            rf.fit(X_train_scaled, y_train)
            rf_score = rf.score(X_test_scaled, y_test)
            cv_scores['rf'].append(rf_score)
            
            # 2. XGBoost (Incremental Ready + tree_method='hist')
            xgb = XGBClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth_xgb,
                learning_rate=learning_rate,
                subsample=subsample,
                colsample_bytree=0.8,
                n_jobs=1,             # Optimizaci√≥n: n_jobs=1 para estabilidad de recursos
                tree_method='hist',   # M√ÅXIMA VELOCIDAD
                random_state=42 + self.training_iteration,
                use_label_encoder=False,
                eval_metric='mlogloss',
                verbosity=0
            )
            
            # üîÑ APRENDIZAJE INCREMENTAL (Rule 3.2): Validar matching de features
            prev_xgb = None
            if hasattr(self, 'xgb_model') and self.xgb_model:
                try:
                    if hasattr(self.xgb_model, 'feature_names_in_') and list(self.xgb_model.feature_names_in_) == list(feature_cols):
                        prev_xgb = self.xgb_model
                        logger.info(f"üîÑ [{self.symbol}] Resuming learning from previous XGBoost model (Incremental).")
                    else:
                        logger.debug(f"üîÑ [{self.symbol}] Feature mismatch for incremental learning. Resetting XGB.")
                except:
                    pass

            xgb.fit(X_train_scaled, y_train, xgb_model=prev_xgb)
            xgb_score = xgb.score(X_test_scaled, y_test)
            cv_scores['xgb'].append(xgb_score)
            
            # 3. Gradient Boosting (Persistent Warm Start - Rule 3.7)
            # PROFESSOR METHOD: Reuse GB instance to accumulate knowledge.
            gb = None
            if hasattr(self, 'gb_model') and self.gb_model is not None:
                gb = self.gb_model
                # Incrementar n_estimators para permitir warm_start real
                gb.n_estimators += 5 
                logger.debug(f"üîÑ [{self.symbol}] Resuming GB with {gb.n_estimators} estimators (Warm Start).")
            else:
                gb = GradientBoostingClassifier(
                    n_estimators=n_estimators,
                    max_depth=max_depth_gb,
                    learning_rate=learning_rate,
                    subsample=subsample,
                    random_state=42 + self.training_iteration,
                    warm_start=True
                )
            
            # The original code had `params.update` here, which is not defined.
            # Assuming it was meant to be part of the parameter setting logic.
            # For now, I'll comment it out or replace with a placeholder if needed.
            # params.update({'n_estimators': 90, 'max_depth_rf': 6, 'learning_rate': 0.06})
            
        try:
            # ‚úÖ PHASE 10: ZERO-COPY SHARED MEMORY IPC
            # Convert to contiguous arrays (float32 for speed)
            X_np = X.to_numpy(dtype=np.float32)
            y_np = y.to_numpy(dtype=np.float32)
            
            # Placeholder for `get_training_pool` and `train_model_process`
            # In a real scenario, these would be defined elsewhere.
            # For this edit, I'll assume they exist and `params` is correctly passed.
            # The `params` variable is not defined in the provided snippet.
            # I'll create a dummy `params` dict for the sake of compilation.
            params = {
                'n_estimators': n_estimators,
                'max_depth_rf': max_depth_rf,
                'max_depth_xgb': max_depth_xgb,
                'max_depth_gb': max_depth_gb,
                'min_samples_split': min_samples_split,
                'learning_rate': learning_rate,
                'subsample': subsample,
                'feature_cols': feature_cols # Pass feature_cols to the worker
            }

            # Dummy functions for compilation
            def get_training_pool():
                class DummyPool:
                    def submit(self, func, *args, **kwargs):
                        class DummyFuture:
                            def result(self, timeout):
                                # Simulate a result
                                return ({'rf': None, 'xgb': None, 'gb': None}, None, [], 0.0, {})
                        return DummyFuture()
                return DummyPool()

            def train_model_process(symbol, X_info, y_info, params, xgb_model, gb_model):
                # This function would typically run in a separate process
                # and perform the actual model training.
                # For this context, it's a placeholder.
                return ({'rf': None, 'xgb': None, 'gb': None}, None, params['feature_cols'], 0.0, {})

            with SharedMemoryManager(X_np) as shm_X, SharedMemoryManager(y_np) as shm_y:
                
                # Metadata payload (Small pickle)
                X_info = {
                    'name': shm_X.name, 
                    'shape': shm_X.shape, 
                    'dtype': shm_X.dtype,
                    'columns': feature_cols
                }
                y_info = {
                    'name': shm_y.name, 
                    'shape': shm_y.shape, 
                    'dtype': shm_y.dtype
                }
                
                # Submit pointer to worker 
                # Note: We must wait for result INSIDE the context manager
                # otherwise SHM is unlinked before worker reads it.
                future = get_training_pool().submit(
                    train_model_process,
                    self.symbol,
                    X_info,  # Pointer
                    y_info,  # Pointer
                    params,
                    getattr(self, 'xgb_model', None),
                    getattr(self, 'gb_model', None)
                )
                
                result = future.result(timeout=180) # Process blocking wait
                
            # SHM is automatically unlinked here __exit__
            
            best_models, best_scaler, f_cols, score, metrics = result
            
            if best_models:
                self.individual_model_scores['rf'] = metrics.get('rf_score', 0)
                self.individual_model_scores['xgb'] = metrics.get('xgb_score', 0)
                self.individual_model_scores['gb'] = metrics.get('gb_score', 0)
                
                logger.info(f"üì• [{self.symbol}] Worker finished. Score: {score:.3f}")
                
                if score >= self.MIN_MODEL_ACCURACY:
                    return (best_models, best_scaler, f_cols), score
            
            return None, score

        except Exception as e:
            logger.error(f"‚ùå Worker/SHM Error for {self.symbol}: {e}")
            return None, 0.0

    # ============================================================
    # ‚úÖ RE-PESADO DIN√ÅMICO DE MODELOS (OMEGA MIND)
    # ============================================================
    
    def update_recursive_weights(self, actual_outcome: float, trade_pnl: float = None, 
                                 duration_seconds: float = 0.0, max_drawdown: float = 0.0, 
                                 axioma_diagnosis: str = "NONE"):
        """
        üöÄ RECURSIVE WEIGHT UPDATE (Phase 9: NEURAL-FORTRESS PPO)
        Uses Asymmetric Reward Shaping and Prioritized Experience Replay.
        """
        if self.online_learner is None or not self.is_trained:
            return

        try:
            # 1. Fallbacks for PnL if not provided (Local Estimation)
            if trade_pnl is None:
                if self.portfolio and self.portfolio.closed_trades:
                    last_trade = self.portfolio.closed_trades[-1]
                    if last_trade['symbol'] == self.symbol:
                        trade_pnl = last_trade.get('pnl_pct', 0.0)
                        duration_seconds = last_trade.get('duration', 0)
                else:
                    trade_pnl = 0.015 if actual_outcome > 0.5 else -0.01

            # 2. Convert string diagnosis to Enum
            enum_diagnosis = TesisDecayReason.NONE
            if "THESIS" in axioma_diagnosis.upper():
                enum_diagnosis = TesisDecayReason.THESIS_DECAY
            elif "CRASH" in axioma_diagnosis.upper() or "DEPTH" in axioma_diagnosis.upper():
                enum_diagnosis = TesisDecayReason.DEPTH_CRASH
            elif "MOMENTUM" in axioma_diagnosis.upper():
                enum_diagnosis = TesisDecayReason.MOMENTUM_REVERSE

            # 3. Calculate Terminal Reward (Non-Linear)
            reward = reward_engine.calculate_reward(
                pnl_pct=trade_pnl,
                max_drawdown_pct=max_drawdown,
                axioma_diagnosis=enum_diagnosis,
                duration_seconds=duration_seconds
            )
            
            # 4. Extract State/Action
            if getattr(self, 'last_ensemble_input', None) is None:
                return # Skip if no inference context

            state = self.last_ensemble_input # [rf_prob, xgb_prob, gb_prob]
            current_weights = np.array([self.base_rf_weight, self.base_xgb_weight, self.base_gb_weight])
            prediction = float(np.dot(current_weights, state))
            
            # PPO Actor Log Prob Approx (Assume Gaussian policy around prediction)
            log_prob = -0.5 * ((prediction - actual_outcome)**2)

            # Add to Prioritized Replay Buffer
            self.memory.add(
                state=state,
                action=prediction,
                reward=reward,
                next_state=np.zeros_like(state), # Terminal bandit state
                log_prob=log_prob,
                axioma_reason=axioma_diagnosis
            )
            
            self.steps_since_learn += 1
            
            # 5. Execute PPO Batch Update
            if self.steps_since_learn >= self.training_batch_size:
                self._learn_ppo_batch()
                self.steps_since_learn = 0
                
        except Exception as e:
            logger.error(f"Neural Fortress PPO update failed: {e}", exc_info=True)

    def _learn_ppo_batch(self):
        """Ejecuta el Clipped Surrogate Objective Update (PPO) sobre el Replay Buffer."""
        try:
            # Sample Black Swans probabilistically
            batch, idxs, weights_is = self.memory.sample(self.training_batch_size)
            if not batch: return
            
            states = np.array([e[0] for e in batch])
            actions = np.array([e[1] for e in batch])
            rewards = np.array([e[2] for e in batch])
            old_log_probs = np.array([e[4] for e in batch])
            advantages = rewards # For Bandit tasks, Advantage ~ Reward
            
            current_weights = np.array([self.base_rf_weight, self.base_xgb_weight, self.base_gb_weight])
            
            # Update via OnlineLearner (Policy tuning)
            new_weights, abs_advantages = self.online_learner.update_ppo_batch(
                weights=current_weights,
                states=states,
                actions=actions,
                old_log_probs=old_log_probs,
                returns=rewards,
                advantages=advantages
            )
            
            # Update priorities back into buffer
            self.memory.update_priorities(idxs, abs_advantages)
            
            # Apply constraint weights
            self._apply_weight_update(new_weights)
            
            logger.info(f"üß† [Neural Fortress] PPO Batch Complete. Avg Reward: {np.mean(rewards):.4f} | Weights Adjusted.")
            
        except Exception as e:
            logger.error(f"PPO Batch Learn Error: {e}", exc_info=True)

    def _apply_weight_update(self, new_weights):
        """Helper to normalize and apply weights."""
        # Normalization and Constraints
        new_weights = np.clip(new_weights, 0.05, 0.70)
        total = np.sum(new_weights)
        if total > 0:
            new_weights = new_weights / total
        
        self.base_rf_weight = float(new_weights[0])
        self.base_xgb_weight = float(new_weights[1])
        self.base_gb_weight = float(new_weights[2])
        
        # Logging
        if self.training_iteration % 10 == 0:
             logger.debug(f"üß† [Omega Weights] UPDATED: RF:{self.base_rf_weight:.2f}, XGB:{self.base_xgb_weight:.2f}, GB:{self.base_gb_weight:.2f}")


    
    def _update_model_weights(self):
        """
        Re-pesado din√°mico basado en performance reciente
        """
        if len(self.performance_history) < 15:
            return
        
        recent_performance = list(self.performance_history)[-15:]
        success_rate = sum(1 for x in recent_performance if x > 0) / len(recent_performance)
        
        # Performance-based weight adjustment
        if success_rate > 0.70:  # Excelente performance
            # Aumentar peso del mejor modelo
            best_model = max(self.individual_model_scores.items(), key=lambda x: x[1])
            weight_increase = 0.05
            
            if best_model[0] == 'rf':
                self.base_rf_weight = min(0.60, self.base_rf_weight + weight_increase)
            elif best_model[0] == 'xgb':
                self.base_xgb_weight = min(0.50, self.base_xgb_weight + weight_increase)
            else:
                self.base_gb_weight = min(0.40, self.base_gb_weight + weight_increase)
                
            logger.debug(f"üî• Increasing {best_model[0]} weight due to excellent performance")
            
        elif success_rate < 0.40:  # Mala performance
            # Volver a pesos originales
            self.base_rf_weight = self.original_rf_weight
            self.base_xgb_weight = self.original_xgb_weight
            self.base_gb_weight = self.original_gb_weight
            logger.debug(f"‚ö†Ô∏è Resetting weights due to poor performance")
        
        # Normalizar pesos
        total = self.base_rf_weight + self.base_xgb_weight + self.base_gb_weight
        self.base_rf_weight /= total
        self.base_xgb_weight /= total
        self.base_gb_weight /= total

    def _adjust_learning_rate(self):
        """
        Ajustar learning rate y factor de agresividad din√°micamente
        """
        if len(self.performance_window) < 10:
            return
            
        recent_perf = list(self.performance_window)
        win_rate = sum(1 for x in recent_perf if x > 0) / len(recent_perf)
        
        # Ajustar learning rate
        if win_rate > 0.65:
            # Buena performance: aumentar agresividad
            self.learning_rate = min(1.3, self.learning_rate * 1.08)
            self.aggressiveness_factor = min(1.5, self.aggressiveness_factor * 1.05)
        elif win_rate < 0.35:
            # Mala performance: reducir agresividad
            self.learning_rate = max(0.7, self.learning_rate * 0.92)
            self.aggressiveness_factor = max(0.7, self.aggressiveness_factor * 0.95)
        else:
            # Performance neutral: tender a 1.0
            self.learning_rate += (1.0 - self.learning_rate) * 0.1
            self.aggressiveness_factor += (1.0 - self.aggressiveness_factor) * 0.1

    # ============================================================
    # ‚úÖ CIRCUIT BREAKER AVANZADO
    # ============================================================
    
    def _check_circuit_breaker(self):
        """
        Verificar condiciones para activar/desactivar circuit breaker
        """
        if not self.portfolio:
            return True
            
        try:
            current_equity = self.portfolio.get_total_equity()
            
            # Inicializar peak equity
            if self.peak_equity is None:
                self.peak_equity = current_equity
            else:
                self.peak_equity = max(self.peak_equity, current_equity)
            
            # Calcular drawdown
            drawdown = (self.peak_equity - current_equity) / self.peak_equity if self.peak_equity > 0 else 0
            
            # Activar por drawdown
            if drawdown > self.circuit_breaker_threshold and not self.circuit_breaker_active:
                self.activate_circuit_breaker()
                return False
            
            # Activar por p√©rdidas consecutivas
            if self.consecutive_losses >= self.max_consecutive_losses and not self.circuit_breaker_active:
                self.activate_circuit_breaker()
                return False
            
            # Desactivar si se recupera
            if self.circuit_breaker_active and drawdown < (self.circuit_breaker_threshold * 0.6):
                self.deactivate_circuit_breaker()
                return True
            
            return not self.circuit_breaker_active
            
        except Exception as e:
            logger.error(f"Error checking circuit breaker: {e}")
            return True

    def activate_circuit_breaker(self):
        """Activar circuit breaker"""
        self.circuit_breaker_active = True
        self.original_confidence_threshold = self.adaptive_confidence_threshold
        self.adaptive_confidence_threshold = min(0.80, self.adaptive_confidence_threshold + 0.25)
        self.aggressiveness_factor = max(0.5, self.aggressiveness_factor * 0.7)
        
        logger.warning(
            f"üî¥ CIRCUIT BREAKER ACTIVATED | "
            f"Confidence: {self.adaptive_confidence_threshold:.2f} | "
            f"Aggressiveness: {self.aggressiveness_factor:.2f}"
        )

    def deactivate_circuit_breaker(self):
        """Desactivar circuit breaker"""
        self.circuit_breaker_active = False
        self.adaptive_confidence_threshold = self.original_confidence_threshold
        self.consecutive_losses = 0
        self.peak_equity = None
        
        logger.info("üü¢ CIRCUIT BREAKER DEACTIVATED")

    # ============================================================
    # ‚úÖ INFERENCE ULTIMATE - ENSEMBLE COMPLETO
    # ============================================================
    
    def _run_inference(self):
        """
        Inference con ensemble completo y prioridad de visibilidad/operatividad.
        """
        try:
            self.analysis_stats['total'] += 1
            if not self._check_circuit_breaker():
                return
            
            # 1. Obtenci√≥n y Preparaci√≥n de Datos
            bars = self.data_provider.get_latest_bars(self.symbol, n=250)
            df = self._prepare_features(bars, regime_aware=True)
            
            if df.empty or len(df) < 5:
                return
                
            current_row = df.iloc[-1]
            atr_pct = current_row['atr_pct'] / 100
            current_atr = current_row['atr']
            rsi = current_row['rsi_14']
            vol_ratio = current_row.get('volume_ratio', 0)
            confluence = current_row['confluence_score']

            # 2. Verificar disponibilidad de Modelos (PRIMERO)
            with self._state_lock:
                models_ready = all([self.rf_model, self.xgb_model, self.gb_model])
                feature_cols = self._feature_cols

            # -------------------------------------------------------------------------
            # ‚úÖ CROSS-POLLINATION (Phase 7): Read Math Stats from Portfolio
            # -------------------------------------------------------------------------
            math_hurst = 0.5
            if self.portfolio and hasattr(self.portfolio, 'math_stats'):
                math_hurst = self.portfolio.math_stats.get('hurst', 0.5)
                
            if not models_ready:
                self.oracle_log_count += 1
                # Log immediately on first check, then periodically to reduce spam
                if self.oracle_log_count == 1 or self.oracle_log_count % 10 == 0:
                    # Determine Concept/Context based on Regime
                    if self.market_regime == "ZOMBIE":
                        concept = "Zombie market detected. Stagnant price action."
                    elif self.market_regime == "RANGING":
                        concept = "Mean Reversion active. Hunting overextensions."
                    elif self.market_regime == "TRENDING":
                        concept = "Trend Following active. Riding momentum."
                    elif self.market_regime == "VOLATILE":
                        concept = "High Volatility. Defensive stops & wide targets."
                    else:
                        concept = "Analyzing market structure..."

                    # Prepare Enhanced Stats for Visibility during Training
                    z_score = current_row.get('volume_zscore', 0)
                    adx = current_row.get('adx', 0)
                    trend_power = current_row.get('trend_power', 0) 

                    oracle_msg = (
                        f"\nüîÆ [UNIFIED ORACLE] {self.symbol} | TRAINING | Last CV: {self.last_training_score:.3f}\n"
                        f"   Engines Passing: 0/3 | Threshold: {self.consensus_threshold}\n"
                        f"   Scores  -> ML: {self.last_training_score:.2f} | SENT: 0.00 | TECH: 0.00\n"
                        f"   Horizon -> H5: 0.00 | H15: 0.00 | H30: 0.00\n"
                        f"   Verdict -> Direction: TRAINING | Final Conf: 0.00 (Gap: 1.00)\n"
                        f"   Phase: {self.market_regime} ({self.regime_confidence*100:.1f}%)\n"
                        f"   Concept: {concept} (Models Compiling)\n"
                        f"   Stats: ADX={adx:.1f} | ATR%={atr_pct*100:.2f}% | Trend={trend_power:.2f} | Z-Score={z_score:.2f}\n"
                        f"   Math: Hurst={math_hurst:.2f} (Portfolio Sync)\n"
                        f"   Confidence: 0.0% | Strategy: Waiting for AI models..."
                    )
                    logger.info(oracle_msg)
                return

            # 2.5 NUEVA: Validaci√≥n cr√≠tica de features
            if feature_cols is None or not feature_cols:
                logger.error(f"‚ùå Error processing {self.symbol}: No valid feature columns available")
                return

            # Filtrar columnas v√°lidas
            valid_features = [col for col in feature_cols if col is not None and col in df.columns]
            if not valid_features:
                logger.error(f"‚ùå Error processing {self.symbol}: No valid features available for inference")
                return

            # 3. Aligned Feature Matrix (Phase 6 Final Fix)
            # Ensure X_pred matches EXACTLY the columns the scaler was fitted on
            if hasattr(self.scaler, 'feature_names_in_'):
                # Strict alignment with scaler
                final_features = self.scaler.feature_names_in_
                # Fill missing if any (shouldn't happen with valid_features but for safety)
                X_pred_aligned = pd.DataFrame(columns=final_features)
                for col in final_features:
                    if col in df.columns:
                        X_pred_aligned[col] = df[col].iloc[[-1]].values
                    else:
                        X_pred_aligned[col] = 0.0
                X_pred = X_pred_aligned
            else:
                # Fallback to old behavior if scaler is legacy
                X_pred = df[valid_features].iloc[[-1]]
                
            if X_pred.empty:
                logger.error(f"‚ùå {self.symbol}: Empty feature matrix after alignment")
                return
                
            if X_pred.isnull().any().any():
                logger.warning(f"‚ö†Ô∏è {self.symbol} has missing values. Using zero-fill.")
                X_pred = X_pred.fillna(0)
                
            X_scaled = self.scaler.transform(X_pred)
            rf_proba = self.rf_model.predict_proba(X_scaled)[0]
            xgb_proba = self.xgb_model.predict_proba(X_scaled)[0]
            gb_proba = self.gb_model.predict_proba(X_scaled)[0]
            
            ensemble_proba = (rf_proba * self.base_rf_weight + 
                             xgb_proba * self.base_xgb_weight + 
                             gb_proba * self.base_gb_weight)
            
            classes = self.rf_model.classes_
            pred_idx = np.argmax(ensemble_proba)
            confidence = ensemble_proba[pred_idx]
            predicted_class = self._label_mapping.get(classes[pred_idx], classes[pred_idx])
            
            # 6. SIGNAL GENERATION (Delegated to Component)
            # Retrieve Dynamic Advice based on Regime
            # NOTE: market_regime object is needed (it's initialized in strategy base or passed in)
            # Assuming self.portfolio.market_regime is available or we need to access the detector
            # Since MLStrategy doesn't hold reference to MarketRegimeDetector directly in __init__,
            # we rely on what was detected in `self.market_regime`.
            
            # Temporary: Get advice using static lookup or if we had the detector instance.
            # Ideally, Strategy should receive the full Regime Context.
            
            from config import Config
            regime_map = getattr(Config.Sniper, 'REGIME_MAP', {})
            advice = regime_map.get(self.market_regime, regime_map.get('RANGING'))
            threshold_mod = advice.get('threshold_mod', 0.0)

            signal_data = self.signal_generator.generate_signal(
                df, 
                prediction=predicted_class, 
                probability=confidence,
                threshold=self.adaptive_confidence_threshold,
                regime=self.market_regime,
                threshold_mod=threshold_mod
            )

            if not signal_data:
                self.analysis_stats['filtered_conf'] += 1
                return
            
            signal_type = signal_data['type']
            final_conf = signal_data['confidence']
            confluence = signal_data['confluence']
            
            # Filtro de Confluencia
            if predicted_class == 1 and confluence < self.adaptive_confluence_long:
                return
            if predicted_class == -1 and confluence > self.adaptive_confluence_short:
                return

            # ============ CREAR SE√ëAL ============
            signal_type = SignalType.LONG if predicted_class == 1 else SignalType.SHORT
            tp_target = self.current_tp_target
            sl_target = self.current_sl_target
            
            # Ajuste de targets por volatilidad
            if atr_pct > 0.03: tp_target *= 1.3; sl_target *= 1.3
            elif atr_pct < 0.01: tp_target *= 0.8; sl_target *= 0.8

            # PHASE 9.2: Adaptive TTL (Prediction Horizon Sync)
            # Default lookup: 30 bars (5m = 150s, 1m = 30s)
            # We use 70% of our lookahead bars as patience.
            prediction_ttl = int(self.LOOKAHEAD_BARS * 60 * 0.7) # e.g. 30 bars * 60s * 0.7 = 1260s
            # Clamp between 30s and 300s for safety in scalping
            final_ttl = max(30, min(300, prediction_ttl))

            # FIXED: Create SignalEvent with ALL metadata in constructor (frozen dataclass)
            signal = SignalEvent(
                strategy_id=self.strategy_id,
                symbol=self.symbol,
                datetime=datetime.now(timezone.utc),
                signal_type=signal_type,
                strength=confidence,
                atr=current_row['atr'], # FIXED: Use current_row
                tp_pct=tp_target * 100,
                sl_pct=sl_target * 100,
                current_price=current_row['close'],
                ttl=final_ttl
            )
            
            # ============ REGISTRAR Y ENVIAR ============
            # Phase 22: XAI Explanation (Why did we decide this?)
            model_used = self.rf_model # Using RF as proxy for explanation (more stable than XGB)
            xai_explanation = self.xai_engine.explain_local_prediction(model_used, X_scaled, "RandomForest")
            
            logger.info(f"üß† [XAI] {signal_type} Signal Reason: {xai_explanation}")
            self.xai_engine.log_trade_explanation(self.symbol, signal_type, xai_explanation)
            
            self.performance_history.append(0)
            self.signal_history.append({
                'timestamp': datetime.now(timezone.utc),
                'type': signal_type,
                'confidence': confidence,
                'regime': self.market_regime,
                'price': current_row['close'],
                'confluence': confluence,
                'xai': xai_explanation
            })
            
            self.total_signals_generated += 1
            if self.market_regime in self.signals_by_regime:
                self.signals_by_regime[self.market_regime] += 1
            
            # Logging completo y env√≠o
            self._log_ml_signal(signal_type, confidence, confluence, df, 
                               rf_proba, xgb_proba, gb_proba)
            
            # Neural Bridge Publication (Base Logic Fallback)
            if not hasattr(self, 'engines_active'): # If not UniversalEnsemble
                neural_bridge.publish_insight(
                    strategy_id="ML_ORACLE",
                    symbol=self.symbol,
                    insight={
                        'confidence': confidence,
                        'direction': 'LONG' if predicted_class == 1 else 'SHORT',
                        'confluence': confluence
                    }
                )

            self.events_queue.put(signal)
            
            # OMEGA MIND: Save input for recursive weighting update
            self.last_ensemble_input = np.array([rf_proba, xgb_proba, gb_proba])
            
            if len(self.performance_history) >= 15:
                self._update_model_weights()
            
            self._last_prediction_time = datetime.now(timezone.utc)
            
        except Exception as e:
            logger.error(f"ML Inference error {self.symbol}: {e}", exc_info=True)

    # ============================================================
    # ‚úÖ LOGGING ULTIMATE - 40+ M√âTRICAS
    # ============================================================
    
    def _log_ml_signal(self, signal_type, confidence, confluence, df, 
                       rf_proba, xgb_proba, gb_proba):
        """
        Logging completo con 40+ m√©tricas para an√°lisis
        """
        try:
            current_row = df.iloc[-1]
            atr_pct = current_row['atr_pct'] / 100
            
            # Preparar todas las m√©tricas
            metrics = {
                # Indicadores principales
                'RSI_14': float(current_row['rsi_14']),
                'RSI_5m': float(current_row.get('rsi_5m', 50)),
                'RSI_15m': float(current_row.get('rsi_15m', 50)),
                'RSI_1h': float(current_row.get('rsi_1h', 50)),
                
                # Volatilidad y tendencia
                'ATR%': float(current_row['atr_pct']),
                'ADX': float(current_row.get('adx', 0)),
                'NATR': float(current_row.get('natr', 0)),
                'MACD_Hist': float(current_row.get('macd_hist', 0)),
                'MACD_Slope': float(current_row.get('macd_slope', 0)),
                
                # Precio y volumen
                'Price_Position': float(current_row.get('close_position', 0.5)),
                'BB_Position': float(current_row.get('bb_position', 0.5)),
                'Volume_Ratio': float(current_row['volume_ratio']),
                'Volume_ZScore': float(current_row.get('volume_zscore', 0)),
                
                # Momentum
                'Momentum_5': float(current_row.get('momentum_5', 0)),
                'Momentum_20': float(current_row.get('momentum_20', 0)),
                'ROC_10': float(current_row.get('roc_10', 0)),
                
                # Tendencia
                'EMA_Cross': int(current_row.get('ema_20_50_cross', 0)),
                'Trend_1h': int(current_row.get('trend_1h', 0)),
                'Trend_1h_Strength': float(current_row.get('trend_1h_strength', 0)),
                
                # Confluence y decisi√≥n
                'Confluence_Score': float(confluence),
                'Prediction_Confidence': float(confidence),
                'Signal_Type': signal_type.name,
                
                # R√©gimen y estado
                'Market_Regime': self.market_regime,
                'Regime_Confidence': float(self.regime_confidence),
                'Regime_Duration': self.regime_duration,
                'Circuit_Breaker': self.circuit_breaker_active,
                'Aggressiveness_Factor': float(self.aggressiveness_factor),
                
                # Scores de modelos
                'Training_Score': float(self.last_training_score),
                'RF_Score': float(self.individual_model_scores.get('rf', 0)),
                'XGB_Score': float(self.individual_model_scores.get('xgb', 0)),
                'GB_Score': float(self.individual_model_scores.get('gb', 0)),
                
                # Pesos del ensemble
                'RF_Weight': float(self.base_rf_weight),
                'XGB_Weight': float(self.base_xgb_weight),
                'GB_Weight': float(self.base_gb_weight),
                
                # Probabilidades individuales
                'RF_Proba_Long': float(rf_proba[1]) if len(rf_proba) > 1 else 0.0,
                'XGB_Proba_Long': float(xgb_proba[1]) if len(xgb_proba) > 1 else 0.0,
                'GB_Proba_Long': float(gb_proba[1]) if len(gb_proba) > 1 else 0.0,
                'Ensemble_Proba_Long': float(rf_proba[1] * self.base_rf_weight + 
                                           xgb_proba[1] * self.base_xgb_weight + 
                                           gb_proba[1] * self.base_gb_weight) if len(rf_proba) > 1 else 0.0,
                
                # Targets
                'TP_Target': float(self.current_tp_target * 100),
                'SL_Target': float(self.current_sl_target * 100),
                'TP_SL_Ratio': float(self.current_tp_target / self.current_sl_target) if self.current_sl_target > 0 else 0,
                
                # Performance
                'Total_Signals': self.total_signals_generated,
                'Win_Rate': (self.winning_trades / self.total_trades) if self.total_trades > 0 else 0,
                'Consecutive_Losses': self.consecutive_losses,
                'Learning_Rate': float(self.learning_rate),
            }
            
            # Log a monitor
            monitor_log.log_ml_prediction(
                symbol=self.symbol,
                model_name="Hybrid_Ensemble_Ultimate",
                prediction=1 if signal_type == SignalType.LONG else -1,
                confidence=float(confidence),
                features=metrics,
                decision=signal_type.name
            )
            
            # Log a consola
            logger.info(
                f"üéØ ML {signal_type.name} {self.symbol} | "
                f"Conf: {confidence:.2f} | Confl: {confluence:.2f} | "
                f"Regime: {self.market_regime} | "
                f"Score: {self.last_training_score:.2f} | "
                f"TP/SL: {self.current_tp_target*100:.1f}%/{self.current_sl_target*100:.1f}% | "
                f"Aggr: {self.aggressiveness_factor:.2f}"
            )
            
            # Debug logging
            logger.debug(
                f"Model Details - "
                f"RF: {rf_proba}, XGB: {xgb_proba}, GB: {gb_proba} | "
                f"Weights: RF={self.base_rf_weight:.2f}, XGB={self.base_xgb_weight:.2f}, GB={self.base_gb_weight:.2f}"
            )
            
        except Exception as e:
            logger.error(f"ML logging error: {e}")

    # ============================================================
    # ‚úÖ ARQUITECTURA AS√çNCRONA OPTIMIZADA
    # ============================================================
    
    def _export_brain_telemetry(self, consensus, votes, weights, status_label):
        """
        [TRINITY] Exports cognitive state to dashboard for Visualization.
        """
        try:
            telemetry = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "consensus_score": float(consensus),
                "votes": {
                    "RL": float(votes[0]),
                    "GA": float(votes[1]),
                    "OL": float(votes[2])
                },
                "weights": {
                    "RL": float(weights[0]),
                    "GA": float(weights[1]),
                    "OL": float(weights[2])
                },
                "status": status_label,
                "entropy": "HIGH" if 0.45 < consensus < 0.55 else "LOW",
                "symbol": self.symbol
            }
            
            # atomic write ideally, but simple replace is fine for dashboard
            path = "dashboard/data/brain_telemetry.json"
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, 'w') as f:
                json.dump(telemetry, f)
        except Exception as e:
            # Non-blocking
            pass

    async def calculate_signals(self, event):
        """Entry point as√≠ncrono (SUPREMO-V3)"""
        if event.type != EventType.MARKET:
            return
        
        # Throttling: m√°ximo 1 predicci√≥n por segundo
        current_time = datetime.now(timezone.utc)
        if (self._last_prediction_time and 
            (current_time - self._last_prediction_time).total_seconds() < 1.0):
            return
        
        # Task in background
        asyncio.create_task(self._async_process_v3(event))

    async def _async_process_v3(self, event):
        """Procesamiento as√≠ncrono sin bloqueo de hilos (SUPREMO-V3)"""
        if not self.running:
            return
            
        try:
            self.loop_count += 1
            required_bars = getattr(Config.Strategies, 'ML_LOOKBACK_BARS', 5000)
            
            # Use to_thread for blocking data retrieval if needed, 
            # though get_latest_bars should be fast if cached.
            bars = await asyncio.to_thread(self.data_provider.get_latest_bars, self.symbol, n=required_bars)
            
            if len(bars) < self.min_bars_to_train:
                return

            if self._feature_cols is None or not self._feature_cols:
                 temp_df = await asyncio.to_thread(self._prepare_features, bars[:1000] if len(bars) > 1000 else bars)
                 if temp_df is not None and not temp_df.empty:
                     self._init_feature_cols(temp_df)

            # Check Training (still in separate thread/process managed by launch_training)
            is_training = (hasattr(self, '_training_thread') and self._training_thread and self._training_thread.is_alive())
            
            if (self.bars_since_train >= self.retrain_interval or not self.is_trained) and not is_training:
                self._launch_training(bars, "Full")
            
            if self.is_trained:
                # Update GARCH with latest return before inference
                if len(bars) > 2:
                    last_ret = np.log(bars[-1][4] / bars[-2][4]) # Log return of Close
                    vol = self.garch.update(last_ret)
                    # self.current_volatility = vol # Optional storage
                
                await self._run_inference_v3(bars)
                
        except Exception as e:
            logger.error(f"ML Async error {self.symbol}: {e}")

    async def _run_inference_v3(self, bars):
        """Inferencia as√≠ncrona via Proceso Aislado (SUPREMO-V3)"""
        try:
            # Prepare features (blocking, move to thread)
            # Use NumPy slicing for Phase 4 Structured Arrays
            data_slice = bars[-100:] if len(bars) > 100 else bars
            df = await asyncio.to_thread(self._prepare_features, data_slice)
            if df is None or df.empty: return
            
            # Predict last row
            last_row = df.iloc[[-1]]
            X = self.scaler.transform(last_row[self._feature_cols])
            
            # Spawn worker if needed
            self._ensure_inference_worker()
            
            # Send to worker process
            self._inference_queue.put({
                'X': X,
                'rf': self.rf_model,
                'xgb': self.xgb_model,
                'gb': self.gb_model,
                'ts': time.time(),
                'symbol': self.symbol,
                'weights': (self.base_rf_weight, self.base_xgb_weight, self.base_gb_weight)
            })
            
            # Check for results (non-blocking)
            try:
                results = self._results_queue.get_nowait()
                await self._process_ml_results(results)
            except Exception:
                pass # No results yet
                
        except Exception as e:
            logger.error(f"Inference Error {self.symbol}: {e}")

    def _ensure_inference_worker(self):
        """Mantiene vivo el subproceso de inferencia"""
        if self._inference_process is None or not self._inference_process.is_alive():
            logger.info(f"üß† [SUPREMO-V3] Starting Isolated Inference Worker for {self.symbol}")
            self._inference_process = multiprocessing.Process(
                target=ml_inference_worker_task,
                args=(self._inference_queue, self._results_queue),
                daemon=True
            )
            self._inference_process.start()

    # ============================================================
    # ‚úÖ PHASE 13: GENETIC ALGORITHM SIGNAL (The Evolver)
    # ============================================================
    def _get_ga_signal(self, symbol):
        """
        Generates a signal based on Evolutionary Technical Analysis.
        Simulates a population of strategies (Genes) and uses the best recent performer.
        Feature: RSI Thresholds, MACD Params, Bollinger Bands deviation.
        """
        try:
            # 1. Fetch recent history (Last 100 bars)
            bars = self.data_provider.get_latest_bars(symbol, n=100)
            if len(bars) < 50: return 0.5
            
            closes = bars['close']
            
            # 2. Define Gene Population (Simplified)
            # Gene: (RSI_Period, RSI_Overbought, RSI_Oversold)
            genes = [
                (14, 70, 30), # Classic
                (7, 80, 20),  # Aggressive
                (21, 65, 35), # Conservative
                (9, 75, 25),  # Scalper
                (5, 85, 15)   # Hyper-Scalper
            ]
            
            best_gene = None
            best_pnl = -999.0
            
            # 3. Evaluate Fitness (Backtest on last 50 bars)
            # We want the gene that would have predicted the *recent* trend best.
            # Simplified: Check RSI divergence with price slope
            
            for gene in genes:
                period, ob, os_lvl = gene
                rsi = talib.RSI(closes, timeperiod=period)
                pnl = 0.0
                
                # Mock backtest
                pos = 0
                entry_price = 0.0
                for i in range(50, len(closes)):
                    price = closes[i]
                    val = rsi[i]
                    
                    if pos == 0:
                        if val < os_lvl: 
                            pos = 1; entry_price = price
                        elif val > ob:
                            pos = -1; entry_price = price
                    elif pos == 1:
                        if val > 50: # Take profit/Exit condition
                            pnl += (price - entry_price) / entry_price
                            pos = 0
                    elif pos == -1:
                        if val < 50:
                            pnl += (entry_price - price) / entry_price
                            pos = 0
                            
                if pnl > best_pnl:
                    best_pnl = pnl
                    best_gene = gene
            
            # 4. Generate Signal using Best Gene (Winner)
            if best_gene:
                period, ob, os_lvl = best_gene
                current_rsi = talib.RSI(closes, timeperiod=period)[-1]
                
                # Normalize 0.0 to 1.0 (0.5 = Neutral)
                # If RSI < OS -> Buy Signal (1.0)
                # If RSI > OB -> Sell Signal (0.0)
                # Middle -> 0.5
                
                if current_rsi <= os_lvl: return 0.9 + (os_lvl - current_rsi)/100 # Strong Buy
                if current_rsi >= ob: return 0.1 - (current_rsi - ob)/100 # Strong Sell
                
                # Linear interpolation
                if current_rsi < 50:
                    return 0.5 + (0.4 * (50 - current_rsi) / (50 - os_lvl))
                else:
                    return 0.5 - (0.4 * (current_rsi - 50) / (ob - 50))
                    
            return 0.5
            
        except Exception as e:
            logger.error(f"GA Signal Error: {e}")
            return 0.5

    async def _process_ml_results(self, results):
        """Handle signals from inference worker (SUPREMO-V3)"""
        try:
            confidence = results['confidence']
            rf_p, xgb_p, gb_p = results['rf'], results['xgb'], results['gb']
            
            # ============================================================
            # ‚úÖ PHASE 13: PHALANX-SWARM CONSENSUS ENGINE
            # ============================================================
            
            # 1. TIMEFRAME DIVERGENCE CHECK (The Chronos Guard)
            # Retrieve momentum features from the dataframe used in inference
            # We need to access the features. 
            # Ideally, they should be passed in 'results' or accessible via cache.
            # Since inference worker is isolated, we need to pass them back or recalc/fetch from feature store.
            # Fast recalc for 1m/5m/15m momentum is cheap.
            
            tf_divergence = False
            try:
                # Quick fetch of recent bars for 3 timeframes
                # Note: DataProvider caching makes this fast.
                mom_1m = self.data_provider.get_latest_bars(self.symbol, n=5)['close']
                mom_5m = self.data_provider.get_latest_bars_5m(self.symbol, n=5)['close']
                mom_15m = self.data_provider.get_latest_bars_15m(self.symbol, n=5)['close']
                
                m1 = (mom_1m[-1] / mom_1m[0]) - 1 if len(mom_1m) > 1 else 0
                m5 = (mom_5m[-1] / mom_5m[0]) - 1 if len(mom_5m) > 1 else 0
                m15 = (mom_15m[-1] / mom_15m[0]) - 1 if len(mom_15m) > 1 else 0
                
                # Check alignment
                if signal_type == SignalType.LONG:
                    if m1 < -0.001 or m5 < -0.001 or m15 < -0.001: # Tolerance of -0.1%
                        tf_divergence = True
                elif signal_type == SignalType.SHORT:
                     if m1 > 0.001 or m5 > 0.001 or m15 > 0.001:
                        tf_divergence = True
                        
                if tf_divergence:
                    logger.info(f"‚è≥ [PHALANX] Timeframe Divergence: M1={m1:.4f} M5={m5:.4f} M15={m15:.4f} -> VETO")
                    confidence *= 0.5 # Penalty, don't kill completely if strong elsewhere
                    
            except Exception as e:
                logger.debug(f"Timeframe check error: {e}")

            # 2. ENSEMBLE CONSENSUS (RL + GA + OL)
            rl_vote = confidence
            ol_vote = rf_p * self.base_rf_weight + xgb_p * self.base_xgb_weight + gb_p * self.base_gb_weight
            ga_vote = self._get_ga_signal(self.symbol)
            
            # ‚úÖ PHASE 17: BYZANTINE FAULT TOLERANCE
            votes = np.array([rl_vote, ga_vote, ol_vote])
            weights = np.array([0.4, 0.3, 0.3]) # Base weights: RL, GA, OL
            names = ['RL', 'GA', 'OL']
            
            # BFT 1: Outlier Detection (The Triad Check)
            mu = np.mean(votes)
            sigma = np.std(votes)
            
            # Only check for traitors if there is significant disagreement
            if sigma > 0.05:
                # Z-Score Calculation
                z_scores = np.abs(votes - mu) / sigma
                
                # Threshold: 1.5 sigma is effective for N=3 to catch the single deviant
                # (User asked for 3 sigma, but that is impossible for N=3. 1.5~=confidence of 86% outlier)
                for i in range(3):
                    if z_scores[i] > 1.5:
                        logger.warning(f"üö´ [BFT] {names[i]} QUARANTINED! Vote={votes[i]:.2f} (Z={z_scores[i]:.2f})")
                        weights[i] = 0.0 # Remove traitor from consensus
            
            # Re-normalize weights
            if np.sum(weights) == 0:
                 weights = np.array([0.4, 0.3, 0.3]) # Fallback if all quarantined (Chaos!)
                 logger.critical("üíÄ [BFT] TOTAL CONSENSUS COLLAPSE. Resetting weights.")
            
            weights /= np.sum(weights)
            consensus_score = np.dot(votes, weights)
            
            logger.info(f"üó≥Ô∏è [BFT] Consensus: {consensus_score:.2f} | Votes: RL={rl_vote:.2f} GA={ga_vote:.2f} OL={ol_vote:.2f} | W={weights}")
            
            # ‚úÖ PHASE III: ENTROPY FILTER (Algorithmic Psychology)
            # If the model is "confused" (near 0.5), we force a HOLD.
            # We want High Conviction only.
            if 0.45 < consensus_score < 0.55:
                logger.info(f"üòµ [PSYCH] High Entropy Detected ({consensus_score:.2f}). Model is confused -> HOLD.")
                return 
            
            if consensus_score < 0.75:
                logger.info(f"üõ°Ô∏è [PHALANX] Consensus VETO ({consensus_score:.2f} < 0.75)")
                self._export_brain_telemetry(consensus_score, votes, weights, "VETO")
                return # Abort Signal
            
            # Boost confidence if Consensus is Strong
            if consensus_score > 0.85:
                confidence = min(confidence + 0.1, 1.0)
            
            self._export_brain_telemetry(consensus_score, votes, weights, "ACTIVE")
            
            # ‚úÖ PHASE 17.3: SPOOFING DETECTION (Injection Filter)
            # Detect Fake Walls: High VBI (>0.8) but Price NOT moving (or moving opposite)
            hft_metrics = self.data_provider.get_hft_indicators(self.symbol)
            vbi = hft_metrics.get('vbi', 0.0)
            
            if abs(vbi) > 0.75:
                # Check recent price velocity
                # If VBI is +0.8 (Strong Buy Wall) but Price is dropping -> SPOOFING (Trap)
                # If VBI is -0.8 (Strong Sell Wall) but Price is rising -> SPOOFING (Trap)
                
                # Simple heuristic: If VBI sign != Momentum sign -> Spoofing Risk
                # reusing 'm1' from Timeframe check if available, else zero
                mom_1m_val = locals().get('m1', 0.0)
                
                if (vbi > 0 and mom_1m_val < -0.0005) or (vbi < 0 and mom_1m_val > 0.0005):
                     logger.warning(f"ü§° [BFT] SPOOFING DETECTED (Fake Wall)! VBI={vbi:.2f} vs Mom={mom_1m_val:.4f}. Penalty applied.")
                     confidence -= 0.20 # Major Penalty
                     consensus_score *= 0.8 # Degrade consensus
                     
            # ‚úÖ PHASE II: LAYERING DETECTION (Microstructure)
            # Detect rapid book changes (VBI Volatility) without price movement
            # Fetches last 10 VBI snapshots via HFT helper if available
            # Note: We rely on HFT metrics returning 'vbi_avg', but we need history/volatility.
            # We assume data_provider can give us VBI history or we compute it if passed in bars? 
            # binance_loader stores vbi_history.
            
            # Using data_provider proxy to access loader's vbi history if possible,
            # Or just infer from current snapshot vs previous (if we tracked it).
            # For strictness: If VBI is extreme (>0.9) and Price Volatility is low (<0.05%), it's Layering.
            
            # Re-using mom_1m_val as proxy for price movement magnitude
            if abs(vbi) > 0.9 and abs(mom_1m_val) < 0.0002: # Huge imbalance, zero movement
                 logger.warning(f"üé≠ [MICROSTRUCTURE] LAYERING DETECTED! Locked Order Book. VBI={vbi:.2f}")
                 confidence -= 0.15
            
            # 3. Proceed to Order Flow Check (Existing Logic)
            
            # [PHASE 13] Absorption Detection (Price Action + Volume)
            # Retrieve last 15 bars for structural analysis
            pa_bars = await asyncio.to_thread(self.data_provider.get_latest_bars, self.symbol, n=15)
            absorption = self.phalanx.is_absorption_detected(pa_bars)
            
            if absorption['detected']:
                logger.info(f"üß± [PHALANX] Absorption Detected: {absorption['type']} ({absorption['reason']})")
            
            # Logic: Imbalance acts as a massive confidence booster or veto
            if signal_type == SignalType.LONG:
                # 1. Order Book Imbalance
                if of_analysis['signal'] == 1: # Long Imbalance > 300%
                     confidence = min(confidence + 0.15, 1.0)
                     logger.info(f"‚ö° [PHALANX] ORACLE LONG BOOST +15% | Strength: {of_analysis['strength']:.2f}")
                elif of_analysis['signal'] == -1: # Short Imbalance -> VETO
                     confidence = max(0.0, confidence - 0.20)
                     logger.info(f"üõ°Ô∏è [PHALANX] ORACLE LONG VETO (Sell Pressure) | Strength: {of_analysis['strength']:.2f}")
                
                # 2. Absorption Confirmation (Stopping Volume at Support)
                if absorption['type'] == 'BULLISH':
                     confidence = min(confidence + 0.10, 1.0)
                     logger.info(f"‚ö° [PHALANX] ABSORPTION BOOST (Bullish Stopping Vol)")
                elif absorption['type'] == 'BEARISH':
                     confidence = max(0.0, confidence - 0.15)
                     logger.info(f"üõ°Ô∏è [PHALANX] ABSORPTION VETO (Resistance blocking)")
                     
            elif signal_type == SignalType.SHORT:
                # 1. Order Book Imbalance
                if of_analysis['signal'] == -1: # Short Imbalance > 300% (Ratio < 0.33)
                     confidence = min(confidence + 0.15, 1.0)
                     logger.info(f"‚ö° [PHALANX] ORACLE SHORT BOOST +15% | Strength: {of_analysis['strength']:.2f}")
                elif of_analysis['signal'] == 1: # Long Imbalance -> VETO
                     confidence = max(0.0, confidence - 0.20)
                     logger.info(f"üõ°Ô∏è [PHALANX] ORACLE SHORT VETO (Buy Pressure) | Strength: {of_analysis['strength']:.2f}")

                # 2. Absorption Confirmation (Stopping Volume at Resistance)
                if absorption['type'] == 'BEARISH':
                     confidence = min(confidence + 0.10, 1.0)
                     logger.info(f"‚ö° [PHALANX] ABSORPTION BOOST (Bearish Stopping Vol)")
                elif absorption['type'] == 'BULLISH':
                     confidence = max(0.0, confidence - 0.15)
                     logger.info(f"üõ°Ô∏è [PHALANX] ABSORPTION VETO (Support blocking)")

            # Logic for Signal Creation (Professor Method)
            # QU√â: Generaci√≥n de evento de se√±al as√≠ncrono.
            # POR QU√â: Para notificar al Portfolio/RiskManager sin bloquear.
            
            # Only act if it's a trade signal
            if signal_type in [SignalType.LONG, SignalType.SHORT]:
                signal = SignalEvent(
                    strategy_id=self.strategy_id,
                    symbol=self.symbol,
                    datetime=datetime.now(timezone.utc),
                    signal_type=signal_type,
                    strength=confidence,
                    current_price=results.get('price', 0)
                )
                
                # Async logging and publishing
                # ... Simplified logging call ...
                await self.events_queue.put(signal)
                
                # Update Neural Bridge insight in background
                neural_bridge.publish_insight(
                    strategy_id="ML_V3_ORACLE",
                    symbol=self.symbol,
                    insight={'confidence': confidence, 'type': signal_type.name}
                )
                
                self._last_prediction_time = datetime.now(timezone.utc)
                self.total_signals_generated += 1
                
        except Exception as e:
            logger.error(f"Error processing ML results: {e}")

    def _init_feature_cols(self, df):
        """
        Inicializa la lista de columnas de features a partir de un DataFrame.
        PROFESSOR METHOD:
        QU√â: M√©todo de inicializaci√≥n de metadatos de entrenamiento.
        POR QU√â: Para asegurar que el modelo siempre use el mismo orden y conjunto de columnas.
        C√ìMO: Filtrando columnas no num√©ricas o reservadas (targets, OHLCV).
        """
        try:
            # Columnas a excluir (Targets y metadatos)
            exclude = [
                'datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume', 
                'target', 'regime', 'timestamp', 'returns'
            ]
            
            # Obtener solo las columnas num√©ricas que no est√°n en la lista de exclusi√≥n
            cols = [c for c in df.columns if c not in exclude and df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]
            
            with self._state_lock:
                self._feature_cols = cols
                
            if not self._feature_cols:
                logger.error(f"‚ùå [{self.symbol}] _init_feature_cols: No valid feature columns found!")
        except Exception as e:
            logger.error(f"‚ùå Error initializing feature columns: {e}")
            self._feature_cols = []

    def _async_process(self, event):
        """Procesamiento en background"""
        if not self.running:
            return
            
        monitor.register_thread(f"ML_{event.symbol}")
        
        try:
            self.loop_count += 1
            # 1. Obtenci√≥n de datos con LOOKBACK CLIP (Rule 2.2)
            required_bars = getattr(Config.Strategies, 'ML_LOOKBACK_BARS', 5000)
            bars = self.data_provider.get_latest_bars(self.symbol, n=required_bars)
            
            # Data Health Check
            num_bars = len(bars)
            if num_bars < self.min_bars_to_train:
                if self.loop_count % 10 == 0:
                    logger.warning(f"‚ö†Ô∏è [{self.symbol}] Waiting for data: {num_bars}/{self.min_bars_to_train} bars.")
                return

            
            # Initial Feature Initialization (Rule 2.3)
            # Preparamos features una vez para inicializar _feature_cols antes del primer training
            # PROFESSOR METHOD:
            # QU√â: Inicializaci√≥n garantizada de metadatos de entrada.
            # POR QU√â: Evita errores de disparidad de dimensiones si el modelo arranca sin saber qu√© columnas usar.
            if self._feature_cols is None or not self._feature_cols:
                 temp_df = self._prepare_features(bars[:1000] if len(bars) > 1000 else bars)
                 if temp_df is not None and not temp_df.empty:
                     self._init_feature_cols(temp_df)
                     if self._feature_cols:
                         logger.info(f"üìã [{self.symbol}] Feature Columns established: {len(self._feature_cols)} features.")
                     else:
                         logger.error(f"‚ùå [{self.symbol}] CRITICAL: _init_feature_cols produced empty list. Check data types.")
                         return
            
            # Actualizar learning rate y par√°metros
            self._adjust_learning_rate()
            
            # Verificar si necesita entrenar (Incremental o Full)
            with self._state_lock:
                self.bars_since_train += 1
                self.bars_since_incremental += 1
                
                # Check if a training thread is already active
                is_training = (hasattr(self, '_training_thread') and 
                               self._training_thread and self._training_thread.is_alive())

                # Update incremental cada X velas (ej. 30)
                needs_incremental = self.bars_since_incremental >= Config.Strategies.ML_INCREMENTAL_UPDATE_BARS
                should_train_full = (
                    (self.bars_since_train >= self.retrain_interval) or
                    (not self.is_trained)
                ) and (not is_training)
                
            if (should_train_full or needs_incremental) and not is_training:
                train_type = "Full" if should_train_full else "Incremental"
                logger.info(f"üîÑ {train_type} training triggered for {self.symbol}")
                self._launch_training(bars, train_type)
            
            # Solo hacer inference si est√° entrenado
            if not self.is_trained:
                return
                
            self._run_inference()
            
        except Exception as e:
            logger.error(f"ML Async error {self.symbol}: {e}", exc_info=True)

    def _launch_training(self, bars, train_type="Full"):
        """Lanzar entrenamiento en thread separado"""
        def train_bg(bars_data, t_type):
            """Core training routine with concurrency control."""
            with TRAINING_LIMITER:
                monitor.register_thread(f"ML_Train_{self.symbol}")
                start_time = time.time()
                
                try:
                    if not self.running:
                        logger.debug(f"ML Training for {self.symbol} aborted: shutdown signal received.")
                        return
                        
                    logger.info(f"üîÑ Starting {t_type} training #{self.training_iteration + 1} for {self.symbol}...")
                    
                    df = self._prepare_features(bars_data, regime_aware=True)
                    result, score = self._train_with_cross_validation(df)
                    
                    if result:
                        models, scaler, feature_cols = result
                        
                        # ‚úÖ GUARDIA DE CALIDAD (Rule 3.3)
                        # PROFESSOR METHOD:
                        # QU√â: Filtro de persistencia basado en performance diferencial.
                        # POR QU√â: Evita que un entrenamiento malo o ruidoso degrade la inteligencia del sistema.
                        # C√ìMO: Comparamos el nuevo score contra el 98% del anterior (umbral de tolerancia).
                        if self.is_trained:
                            if score < (self.last_training_score * 0.98):
                                logger.warning(
                                    f"üõ°Ô∏è [Quality Guard] Rejected model update for {self.symbol}.\n"
                                    f"   Current Score: {score:.4f} | Previous Score: {self.last_training_score:.4f}\n"
                                    f"   Reason: Performance degradation exceeds 2% threshold."
                                )
                                # NO actualizamos, pero limpiamos flag de entrenamiento
                                with self._state_lock:
                                    self.bars_since_train = 0
                                return

                        if not self.running:
                            return
                            
                        with self._state_lock:
                            # Guardar los 3 modelos
                            self.rf_model = models['rf']
                            self.xgb_model = models['xgb']
                            self.gb_model = models['gb']
                            self.scaler = scaler
                            cleaned_feature_cols = [col for col in feature_cols if col is not None]
                            self._feature_cols = cleaned_feature_cols
                            self.is_trained = True
                            self.bars_since_train = 0
                            self.last_training_score = score
                            self.last_training_time = datetime.now(timezone.utc)
                            self.training_iteration += 1
                            self.bars_since_incremental = 0
                        
                        # üìä PERSISTENCIA INTELIGENTE (Rule 3.4)
                        self._save_models()
                        
                        duration = time.time() - start_time
                        logger.info(
                            f"‚ú®‚ú®‚ú® [ML {self.symbol}] {t_type.upper()} TRAINING FINISHED ‚ú®‚ú®‚ú®\n"
                            f"   Result: SUCCESS #{self.training_iteration} | Score: {score:.3f} | Total Time: {duration:.1f}s\n"
                            f"   Features Used: {len(feature_cols)} | Quality Guard: PASSED"
                        )
                    else:
                        # Even if training result is None (score below threshold),
                        # we must mark it as trained to avoid infinite re-train loops every minute.
                        # It will try again in the next retrain_interval.
                        with self._state_lock:
                            self.is_trained = True 
                            self.bars_since_train = 0
                            self.last_training_score = score
                            self.phalanx = OrderFlowAnalyzer(window=20)
                            self.garch = OnlineGARCH() 
                            
                            # Phase I: Feature Engineering Component
                            self.feature_engineer = FeatureEngineering()
                            self.signal_generator = SignalGenerator(self.strategy_id)
                            self.xai_engine = XAIEngine() # Phase 22: Explainability

                            self.last_prediction_time = 0 
                            self.prediction_interval = 60 # 1 minute 
                            
                            # Phase 6: Thread-Safe State Management
                        logger.warning(
                            f"‚ö†Ô∏è ML {self.symbol} training failed (Score: {score:.3f} < {self.MIN_MODEL_ACCURACY}). "
                            "Marking as trained to wait for next interval."
                        )
                        
                except Exception as e:
                    logger.error(f"ML Training error {self.symbol}: {e}", exc_info=True)
                finally:
                    # MODO PROFESOR: Liberar RAM agresivamente
                    gc.collect()
        
        self._training_thread = threading.Thread(
            target=train_bg, 
            args=(copy.deepcopy(bars), train_type),
            daemon=True
        )
        self._training_thread.start()

    def stop(self):
        """
        Signal the strategy to stop processing and cleanup resources.
        """
        logger.info(f"üõë [ML {self.symbol}] Stopping strategy...")
        self.running = False
        
        # Shutdown thread pool executor
        try:
            self.executor.shutdown(wait=False)
        except Exception as e:
            logger.debug(f"Error shutting down executor: {e}")
        
        # We don't force-join the training thread here to avoid hanging the main shutdown,
        # but the running flag inside the training thread will handle the early exit.

    # ============================================================
    # ‚úÖ GESTI√ìN DE TRADES Y ACTUALIZACI√ìN DE PERFORMANCE
    # ============================================================
    
    def update_trade_result(self, signal_id, success, profit_pct=0.0):
        """
        Actualizar resultado de un trade para aprendizaje continuo
        """
        try:
            # Actualizar historial de performance
            result_value = 1 if success else -1
            if len(self.performance_history) > 0:
                self.performance_history[-1] = result_value
            
            # Actualizar ventana de performance
            self.performance_window.append(1 if success else 0)
            
            # Actualizar streaks
            if success:
                self.consecutive_losses = 0
                self.win_streak += 1
                self.loss_streak = 0
                self.winning_trades += 1
                self.max_win_streak = max(self.max_win_streak, self.win_streak)
            else:
                self.consecutive_losses += 1
                self.loss_streak += 1
                self.win_streak = 0
                self.losing_trades += 1
                self.max_loss_streak = max(self.max_loss_streak, self.loss_streak)
            
            self.total_trades += 1
            
            # Actualizar accuracy por r√©gimen
            if self.market_regime in self.regime_accuracy:
                self.regime_accuracy[self.market_regime].append(1 if success else 0)

        except Exception as e:
            logger.error(f"Error updating trade result: {e}")

    # ============================================================
    # ‚úÖ PERSISTENCIA DE MODELOS EN DISCO
    # ============================================================
    
    def _save_models(self):
        """Guardar modelos y scaler con sistema de rotaci√≥n backup (Rule 3.4)"""
        import joblib
        try:
            symbol_path = self.symbol.replace("/", "_")
            model_file = os.path.join(self.models_dir, f"models_{symbol_path}.joblib")
            
            # Sistema de Backup Rotativo
            if os.path.exists(model_file):
                old_path = model_file + ".old"
                if os.path.exists(old_path):
                    os.remove(old_path)
                os.rename(model_file, old_path)

            with self._state_lock:
                state = {
                    'rf_model': self.rf_model,
                    'xgb_model': self.xgb_model,
                    'gb_model': self.gb_model,
                    'scaler': self.scaler,
                    'feature_cols': self._feature_cols,
                    'last_training_score': self.last_training_score,
                    'training_iteration': self.training_iteration,
                    'performance_history': list(self.performance_history),
                    'timestamp': datetime.now(timezone.utc)
                }
            
            joblib.dump(state, model_file, compress=5)
            
            # === [PHASE 12] REGISTRO EN GOBERNANZA ===
            # Exportamos componentes individuales para el registry
            reg_dir = os.path.join(self.models_dir, f"registry_{symbol_path}")
            os.makedirs(reg_dir, exist_ok=True)
            
            comp_paths = {
                'rf': os.path.join(reg_dir, "rf.joblib"),
                'xgb': os.path.join(reg_dir, "xgb.joblib"),
                'gb': os.path.join(reg_dir, "gb.joblib"),
                'scaler': os.path.join(reg_dir, "scaler.joblib")
            }
            
            joblib.dump(self.rf_model, comp_paths['rf'])
            joblib.dump(self.xgb_model, comp_paths['xgb'])
            joblib.dump(self.gb_model, comp_paths['gb'])
            joblib.dump(self.scaler, comp_paths['scaler'])
            
            metrics = {
                'sharpe': self.last_training_score,
                'win_rate': 0.0
            }
            if len(self.performance_history) > 0:
                win_rate = len([p for p in self.performance_history if p > 0]) / len(self.performance_history)
                metrics['win_rate'] = win_rate * 100

            self.ml_governance.register_model(self.symbol, metrics, comp_paths)
            
            logger.info(f"üíæ [{self.symbol}] Models persisted and registered in Governance.")
        except Exception as e:
            logger.error(f"Error saving models for {self.symbol}: {e}")
    def _load_governed_model(self):
        """
        üë®‚Äçüè´ MODO PROFESOR:
        QU√â: Carga inteligente de modelos certificados.
        POR QU√â: Priorizamos modelos que han pasado el Quality Gate de la gobernanza.
        """
        import joblib
        gov_model = self.ml_governance.get_production_model(self.symbol)
        if gov_model:
            try:
                path = gov_model['path']
                self.rf_model = joblib.load(os.path.join(path, "rf.joblib"))
                self.xgb_model = joblib.load(os.path.join(path, "xgb.joblib"))
                self.gb_model = joblib.load(os.path.join(path, "gb.joblib"))
                self.scaler = joblib.load(os.path.join(path, "scaler.joblib"))
                
                # Cargar columnas (buscamos en models_dir original por ahora o el √∫ltimo cache)
                cols_path = os.path.join(self.models_dir, f"features_{self.symbol.replace('/', '_')}.json")
                if os.path.exists(cols_path):
                    with open(cols_path, 'r') as f:
                        self._feature_cols = json.load(f)
                
                self.is_trained = True
                logger.info(f"üèÜ [{self.symbol}] Cargado modelo de PRODUCCI√ìN v{gov_model['version']} (Sharpe: {gov_model['sharpe']:.2f})")
                return True
            except Exception as e:
                logger.error(f"‚ùå Error cargando modelo de gobernanza: {e}")
        
        # Fallback a carga tradicional si no hay modelo de gobernanza
        return self._load_models()

    def _load_models(self):
        """Cargar modelos desde el disco para operatividad instant√°nea"""
        try:
            symbol_path = self.symbol.replace("/", "_")
            model_file = os.path.join(self.models_dir, f"models_{symbol_path}.joblib")
            
            # --- SUPREME PROTOCOL: XGBoost JSON Support ---
            # Phase 26-30 trained models are in 'models/{symbol}_xgb.json'
            # We check this FIRST as it is the freshest intelligence
            safe_sym = self.symbol.replace('/', '')
            xgb_json_path = os.path.join("models", f"{safe_sym}_xgb.json")
            
            supreme_loaded = False
            if os.path.exists(xgb_json_path):
                try:
                    self.xgb_model = XGBClassifier()
                    self.xgb_model.load_model(xgb_json_path)
                    self.is_trained = True
                    supreme_loaded = True
                    # Initialize dummies for others to prevent crashes if code expects them, 
                    # but we will relax inference check logic next.
                    self.rf_model = None 
                    self.gb_model = None
                    # Scaler? train_supreme didn't save scaler separately, 
                    # it used raw features or implicit scaling? 
                    # train_supreme used calculate_rsi_jit etc on raw float32. 
                    # It constructed X directly. It did NOT use a scaler in the final save!
                    # Wait, train_supreme.py did NOT save a scaler. 
                    # But the strategy _prepare_features generates 80+ features.
                    # train_supreme.py used specific features: RSI, ZScore, Returns.
                    # Mismatch! 
                    # train_supreme.py trained on [rsi, zscore, returns].
                    # ml_strategy.py generates 80 features.
                    # If I load the model, it expects 3 features. 
                    # The strategy passes 80 features.
                    # This will CRASH at prediction time due to shape mismatch.
                    
                    logger.info(f"üü¢ [{self.symbol}] SUPREME XGBoost Model Loaded (JSON). Warning: Feature set mismatch potential.")
                except Exception as e:
                     logger.error(f"Failed to load Supreme JSON: {e}")

            if supreme_loaded:
                return

            if os.path.exists(model_file):
                state = joblib.load(model_file)
                save_time = state.get('timestamp')
                
                # --- VERIFICACI√ìN DE CADUCIDAD (FRESHNESS) ---
                is_stale = False
                if save_time:
                    age_hours = (datetime.now(timezone.utc) - save_time).total_seconds() / 3600
                    if age_hours > 24:
                        is_stale = True
                        logger.warning(f"‚ö†Ô∏è [{self.symbol}] Intelligence STALE ({age_hours:.1f}h old). Checking transfer learning...")
                
                # ‚úÖ VALIDACI√ìN DE FEATURES
                loaded_feature_cols = state.get('feature_cols', [])
                cleaned_feature_cols = [col for col in loaded_feature_cols if col is not None]
                
                # ‚úÖ CRITICAL: Si no hay features v√°lidas, el modelo es corrupto
                if not cleaned_feature_cols:
                    logger.error(f"‚ùå [{self.symbol}] Corrupted model detected (no valid features). Checking transfer learning...")
                    is_stale = True
                
                if not is_stale:
                    with self._state_lock:
                        self.rf_model = state['rf_model']
                        self.xgb_model = state['xgb_model']
                        self.gb_model = state['gb_model']
                        self.scaler = state['scaler']
                        self._feature_cols = cleaned_feature_cols
                        self.last_training_score = state.get('last_training_score', 0)
                        self.training_iteration = state.get('training_iteration', 0)
                        
                        hist = state.get('performance_history', [])
                        self.performance_history = deque(hist, maxlen=100)
                        
                        self.is_trained = True
                    
                    logger.info(f"üü¢ [{self.symbol}] ML HYBRID ULTIMATE [ENSEMBLE] - Native model loaded")
                    return
                else:
                    use_transfer_learning = True
            else:
                use_transfer_learning = True
            
            # --- TRANSFER LEARNING FROM BTC ---
            if use_transfer_learning and self.symbol != 'BTC/USDT' and os.path.exists(btc_model_file):
                try:
                    btc_state = joblib.load(btc_model_file)
                    btc_features = btc_state.get('feature_cols', [])
                    cleaned_btc_features = [col for col in btc_features if col is not None]
                    
                    if cleaned_btc_features and btc_state.get('rf_model'):
                        with self._state_lock:
                            self.rf_model = btc_state['rf_model']
                            self.xgb_model = btc_state['xgb_model']
                            self.gb_model = btc_state['gb_model']
                            self.scaler = btc_state['scaler']
                            self._feature_cols = cleaned_btc_features
                            self.is_trained = True
                            self.training_iteration = 0  # Mark for retraining
                        
                        logger.info(f"üü¢ [{self.symbol}] ML HYBRID ULTIMATE [ENSEMBLE] - Transfer learned from BTC")
                        return
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [{self.symbol}] Transfer learning failed: {e}")
            
            logger.info(f"üü¢ [{self.symbol}] ML HYBRID ULTIMATE [ENSEMBLE] - Fresh training starting...")
            
        except Exception as e:
            logger.error(f"Error loading models for {self.symbol}: {e}")

    # ============================================================
    # ‚úÖ M√âTODOS DE MONITOREO Y DIAGN√ìSTICO
    # ============================================================

    
    def get_strategy_status(self):
        """
        Obtener estado completo de la estrategia
        """
        try:
            # Calcular m√©tricas
            recent_win_rate = 0.0
            if len(self.performance_window) > 0:
                recent_win_rate = sum(self.performance_window) / len(self.performance_window)
            
            total_win_rate = 0.0
            if self.total_trades > 0:
                total_win_rate = self.winning_trades / self.total_trades
            
            # Accuracy por r√©gimen
            regime_stats = {}
            for regime, results in self.regime_accuracy.items():
                if len(results) > 0:
                    regime_stats[regime] = {
                        'accuracy': sum(results) / len(results),
                        'signals': len(results),
                        'total_signals': self.signals_by_regime.get(regime, 0)
                    }
                else:
                    regime_stats[regime] = {'accuracy': 0.0, 'signals': 0, 'total_signals': 0}
            
            # Feature importance si est√° disponible
            feature_importance = {}
            if self.rf_model is not None and self._feature_cols is not None:
                try:
                    importances = self.rf_model.feature_importances_
                    top_features = dict(zip(self._feature_cols, importances))
                    sorted_features = sorted(top_features.items(), key=lambda x: x[1], reverse=True)[:20]
                    feature_importance = dict(sorted_features)
                except:
                    feature_importance = {}
            
            return {
                # Identificaci√≥n
                'strategy_id': self.strategy_id,
                'symbol': self.symbol,
                'objective': f"${self.initial_capital} ‚Üí ${self.target_capital}",
                
                # Estado general
                'trained': self.is_trained,
                'training_iteration': self.training_iteration,
                'training_score': self.last_training_score,
                'last_training': self.last_training_time.isoformat() if self.last_training_time else None,
                'bars_since_train': self.bars_since_train,
                'feature_count': len(self._feature_cols) if self._feature_cols else 0,
                
                # R√©gimen de mercado
                'market_regime': self.market_regime,
                'regime_confidence': self.regime_confidence,
                'regime_duration': self.regime_duration,
                'regime_history': list(self.regime_history)[-10:],
                'regime_stats': regime_stats,
                
                # Circuit breaker
                'circuit_breaker_active': self.circuit_breaker_active,
                'consecutive_losses': self.consecutive_losses,
                'max_consecutive_losses': self.max_consecutive_losses,
                'peak_equity': self.peak_equity,
                
                # Modelos y ensemble
                'model_weights': {
                    'rf': float(self.base_rf_weight),
                    'xgb': float(self.base_xgb_weight),
                    'gb': float(self.base_gb_weight)
                },
                'model_scores': {k: float(v) for k, v in self.individual_model_scores.items()},
                'feature_importance': feature_importance,
                
                # Par√°metros adaptativos
                'adaptive_parameters': {
                    'confidence_threshold': float(self.adaptive_confidence_threshold),
                    'confluence_long': float(self.adaptive_confluence_long),
                    'confluence_short': float(self.adaptive_confluence_short),
                    'learning_rate': float(self.learning_rate),
                    'aggressiveness_factor': float(self.aggressiveness_factor)
                },
                
                # Targets
                'targets': {
                    'tp': float(self.current_tp_target),
                    'sl': float(self.current_sl_target),
                    'tp_sl_ratio': float(self.current_tp_target / self.current_sl_target) if self.current_sl_target > 0 else 0
                },
                
                # Performance completa
                'performance': {
                    'total_signals': self.total_signals_generated,
                    'total_trades': self.total_trades,
                    'winning_trades': self.winning_trades,
                    'losing_trades': self.losing_trades,
                    'total_win_rate': float(total_win_rate),
                    'recent_win_rate': float(recent_win_rate),
                    'max_win_streak': self.max_win_streak,
                    'max_loss_streak': self.max_loss_streak,
                    'current_win_streak': self.win_streak,
                    'current_loss_streak': self.loss_streak,
                    'compounding_factor': float(self.compounding_factor)
                },
                
                # Signals por r√©gimen
                'signals_by_regime': self.signals_by_regime,
                
                # Historial reciente
                'recent_performance': list(self.performance_history)[-20:],
                'recent_signals': [
                    {
                        'time': s['timestamp'].isoformat(),
                        'type': s['type'].name,
                        'confidence': s['confidence'],
                        'regime': s['regime'],
                        'price': s['price'],
                        'confluence': s['confluence']
                    }
                    for s in list(self.signal_history)[-10:]
                ],
                
                # Estado del sistema
                'thread_alive': self._training_thread.is_alive() if hasattr(self, '_training_thread') and self._training_thread else False,
                'last_prediction': self._last_prediction_time.isoformat() if self._last_prediction_time else None,
                'current_capital': float(self.current_capital),
                'progress_percentage': float((self.current_capital - self.initial_capital) / (self.target_capital - self.initial_capital) * 100) if self.target_capital > self.initial_capital else 0.0
            }
            
        except Exception as e:
            logger.error(f"Error getting strategy status: {e}")
            return {'error': str(e)}

    def get_performance_report(self):
        """
        Generar reporte detallado de performance
        """
        status = self.get_strategy_status()
        
        report = {
            'summary': {
                'symbol': status['symbol'],
                'trained': status['trained'],
                'total_trades': status['performance']['total_trades'],
                'win_rate': f"{status['performance']['total_win_rate']*100:.1f}%",
                'recent_win_rate': f"{status['performance']['recent_win_rate']*100:.1f}%",
                'current_regime': status['market_regime'],
                'circuit_breaker': status['circuit_breaker_active']
            },
            'model_info': {
                'training_score': f"{status['training_score']:.3f}",
                'ensemble_weights': status['model_weights'],
                'model_scores': status['model_scores']
            },
            'parameters': status['adaptive_parameters'],
            'targets': status['targets'],
            'streaks': {
                'current_win_streak': status['performance']['current_win_streak'],
                'current_loss_streak': status['performance']['current_loss_streak'],
                'max_win_streak': status['performance']['max_win_streak'],
                'max_loss_streak': status['performance']['max_loss_streak']
            },
            'regime_performance': status['regime_stats']
        }
        
        return json.dumps(report, indent=2)

    def force_retrain(self):
        """Forzar reentrenamiento inmediato"""
        with self._state_lock:
            self.is_trained = False
            self.bars_since_train = self.retrain_interval
        logger.info(f"üîß Forced retrain triggered for {self.symbol}")

    def reset_weights(self):
        """Resetear pesos a valores originales"""
        self.base_rf_weight = self.original_rf_weight
        self.base_xgb_weight = self.original_xgb_weight
        self.base_gb_weight = self.original_gb_weight
        logger.info("üîÑ Model weights reset to original values")

    def reset_circuit_breaker(self):
        """Resetear circuit breaker manualmente"""
        self.circuit_breaker_active = False
        self.consecutive_losses = 0
        self.adaptive_confidence_threshold = self.BASE_CONFIDENCE_THRESHOLD
        self.peak_equity = None
        self.aggressiveness_factor = 1.0
        logger.info("üîÑ Circuit breaker manually reset")

    def set_aggressiveness(self, level):
        """
        Ajustar nivel de agresividad manualmente
        Level: 0.5 (conservador), 1.0 (normal), 1.5 (agresivo), 2.0 (muy agresivo)
        """
        level = max(0.3, min(2.0, level))
        self.aggressiveness_factor = level
        logger.info(f"üîß Aggressiveness set to {level:.2f}")

    # ============================================================
    # ‚úÖ CLEANUP Y DESTRUCTOR
    # ============================================================
    
    def __del__(self):
        """Cleanup seguro"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            
            logger.info(f"üßπ ML Hybrid Ultimate Strategy for {self.symbol} cleaned up")
        except:
            pass

# ============================================================
# ‚úÖ UNIVERSAL ENSEMBLE STRATEGY - UNIFIED DECISION MAKING
# ============================================================

class UniversalEnsembleStrategy(MLStrategyHybridUltimate):
    """
    üîÆ UNIVERSAL ENSEMBLE STRATEGY
    
    PROFESSOR METHOD:
    - QU√â: Estrategia unificada que combina 3 motores de decisi√≥n
    - POR QU√â: Elimina modelos heredados y fuerza consenso org√°nico
    - C√ìMO: Promedia se√±ales de ML, Sentiment y Technical
    - CU√ÅNDO: 2 de 3 motores deben superar 0.60 para operar
    - D√ìNDE: Reemplaza cualquier l√≥gica de "Previous models"
    
    Engines:
    1. ML Engine: RF + XGB + GB ensemble inference
    2. Sentiment Engine: VADER/Social momentum analysis
    3. Technical Engine: RSI + EMA Cross + Bollinger confluence
    
    Consensus: Organic Confluence (min 2/3 engines > 0.60)
    """
    
    # Consensus threshold (UNIFIED)
    ENSEMBLE_CONSENSUS_THRESHOLD = 0.75
    MIN_ENGINES_REQUIRED = 2      # Requires at least 2 engines to agree (ML+TECH, ML+SNT, etc)
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # Override strategy ID for ENSEMBLE mode
        self.strategy_id = "ML_HYBRID_ULTIMATE_ENSEMBLE_V3"
        
        # Engine tracking
        self.engine_scores = {
            'ml': 0.0,
            'sentiment': 0.0,
            'technical': 0.0
        }
        self.engines_active = 0
        self.consensus_threshold = 0.60
        
        logger.info(f"üü¢ [{self.symbol}] UNIVERSAL ENSEMBLE STRATEGY INITIALIZED")
        logger.info(f"   Consensus Threshold: {self.ENSEMBLE_CONSENSUS_THRESHOLD}")
        logger.info(f"   Min Engines Required: {self.MIN_ENGINES_REQUIRED}/3")

    def _run_inference(self):
        """
        üîÆ UNIVERSAL ENSEMBLE INFERENCE
        Overridden to force 3-engine consensus and bridge validation.
        """
        try:
            self.analysis_stats['total'] += 1
            if not self._check_circuit_breaker():
                return
            
            # 1. Data Preparation
            bars = self.data_provider.get_latest_bars(self.symbol, n=250)
            df = self._prepare_features(bars, regime_aware=True)
            if df.empty or len(df) < 5: return

            current_row = df.iloc[-1]
            atr_pct = current_row['atr_pct'] / 100
            vol_ratio = current_row.get('volume_ratio', 0)

            # 2. Model Availability
            with self._state_lock:
                # SUPREME MODE DETECTED: If only XGB is loaded and rf/gb are None
                supreme_mode = (self.xgb_model is not None and self.rf_model is None)
                models_ready = all([self.rf_model, self.xgb_model, self.gb_model]) or supreme_mode
                feature_cols = self._feature_cols

            if not models_ready:
                return

            # 3. Aligned Feature Matrix
            if supreme_mode:
                 # --- SUPREME FEATURE ADAPTER ---
                 # train_supreme.py used: rsi_14, zscore_20, log_returns
                 # We must reconstruct these exactly.
                 # df has 'close' as float (from _prepare_features)
                 closes = df['close'].values.astype(np.float64)
                 
                 # RSI 14
                 rsi = talib.RSI(closes, timeperiod=14)
                 
                 # Z-Score 20 (Manual calculation to match math_kernel)
                 roll_mean = pd.Series(closes).rolling(20).mean()
                 roll_std = pd.Series(closes).rolling(20).std(ddof=0)
                 zscore = (closes - roll_mean) / roll_std
                 zscore = zscore.fillna(0).values
                 
                 # Log Return
                 # np.log(price / prev_price)
                 returns = np.diff(np.log(closes), prepend=np.log(closes[0]))
                 
                 # Stack for last row
                 # Features: [rsi, zscore, returns]
                 # We take the last element
                 curr_rsi = rsi[-1]
                 curr_z = zscore[-1]
                 curr_ret = returns[-1]
                 
                 X_pred = np.array([[curr_rsi, curr_z, curr_ret]], dtype=np.float32)
                 
                 # Predict
                 # No scaler used in train_supreme (raw features)
                 xgb_proba = self.xgb_model.predict_proba(X_pred)[0]
                 
                 # Mock others for organic confluence logic, or bypass
                 # Since Organic Confluence expects 3 engines...
                 # We will trust XGB fully in Supreme Mode.
                 
                 pred_idx = np.argmax(xgb_proba)
                 final_confidence = xgb_proba[pred_idx]
                 # classes: [0, 1] which map to [-1, 1] via label_mapping
                 raw_cls = self.xgb_model.classes_[pred_idx]
                 # Map 0->-1, 1->1 (if trained with 0,1)
                 # train_supreme used: 0=DOWN, 1=UP.
                 # Strategy expects: -1=SHORT, 1=LONG
                 direction = 1 if raw_cls == 1 else -1
                 
                 # Log Oracle
                 if final_confidence > 0.55: # Min threshold
                     logger.info(f"üîÆ [SUPREME ORACLE] {self.symbol} Signal: {direction} (Conf: {final_confidence:.2f})")
                     # Trigger Signal
                     self.events_queue.put(SignalEvent(
                        symbol=self.symbol,
                        timestamp=datetime.now(timezone.utc),
                        signal_type=SignalType.LONG if direction == 1 else SignalType.SHORT,
                        prob=final_confidence,
                        strategy_id="SUPREME_XGB_V1",
                        price=bars[-1]['close'],
                        stop_loss=bars[-1]['close'] * (1 - 0.005 if direction==1 else 1 + 0.005),
                        take_profit=bars[-1]['close'] * (1 + 0.015 if direction==1 else 1 - 0.015),
                        metadata={'source': 'SUPREME_XGB'}
                     ))
                 return

            if hasattr(self.scaler, 'feature_names_in_'):
                final_features = self.scaler.feature_names_in_
                X_pred_aligned = pd.DataFrame(columns=final_features)
                for col in final_features:
                    X_pred_aligned[col] = df[col].iloc[[-1]].values if col in df.columns else 0.0
                X_pred = X_pred_aligned
            else:
                X_pred = df[feature_cols].iloc[[-1]]
                
            X_scaled = self.scaler.transform(X_pred)
            rf_proba = self.rf_model.predict_proba(X_scaled)[0]
            xgb_proba = self.xgb_model.predict_proba(X_scaled)[0]
            gb_proba = self.gb_model.predict_proba(X_scaled)[0]
            
            ensemble_proba = (rf_proba * self.base_rf_weight + 
                             xgb_proba * self.base_xgb_weight + 
                             gb_proba * self.base_gb_weight)
            
            classes = self.rf_model.classes_
            pred_idx = np.argmax(ensemble_proba)
            raw_confidence = ensemble_proba[pred_idx]
            direction = self._label_mapping.get(classes[pred_idx], classes[pred_idx])
            
            # Phase 9: Capture Ensemble Input for PPO (Probabilities of CHOSEN direction)
            # We need the probabilities corresponding to the '1' class (Up) or '0' (Down), depending on how we model state.
            # But `update_recursive_weights` uses `last_ensemble_input` to compute `dot` product.
            # If direction is 1 (LONG), we want predicted prob of LONG.
            # If direction is -1 (SHORT), we want predicted prob of SHORT?
            # Actually, `last_ensemble_input` should be the raw inputs to the meta-learner.
            # The meta-learner weights [w1, w2, w3] allow it to trust RF, XGB, GB differently.
            # So `last_ensemble_input` should be the confidence of each model in the CHOSEN direction.
            
            chosen_idx = 1 if direction == 1 else 0 # Assuming 1=LONG, 0=SHORT in probas (check classes_)
            # If classes_ are [-1, 1], then index for -1 is 0, for 1 is 1.
            # If classes_ are [0, 1], then index for 0 is 0, for 1 is 1.
            
            # Map direction to index
            ml_idx = 0
            if hasattr(self.rf_model, 'classes_'):
                 # Find index of 'direction' in classes_
                 # If direction is not in classes (e.g. mapped), we need to revert mapping?
                 # _label_mapping maps internal class to 1/-1.
                 # Let's assume binary classifier.
                 if direction == 1: ml_idx = 1
                 else: ml_idx = 0
            
            self.last_ensemble_input = np.array([
                rf_proba[ml_idx],
                xgb_proba[ml_idx],
                gb_proba[ml_idx]
            ])
            
            # Store state for PPO (Observation)
            # We store the inputs to the ensemble (model probs) + some key features?
            # Or just the inputs to the weighting layer?
            # The "Policy" here is the weighting [w1, w2, w3].
            # The state for PPO should be relevant to "which model is right?".
            # For now, we use the model probs as state.
            self.last_ppo_state = self.last_ensemble_input.copy()
            self.last_ppo_action_probs = ensemble_proba # Full prob dist
            
            # ============================================================
            # üéØ 3-ENGINE ORGANIC CONFLUENCE (The Heart of Phase 8)
            # ============================================================
            final_confidence, engines_passing, is_valid, multi_horizon = self.compute_organic_confluence(
                df, direction, rf_proba, xgb_proba, gb_proba
            )
            
            # 4. ORACLE REPORT (Universal Multi-Engine View)
            gap = self.consensus_threshold - final_confidence
            ready_status = "READY" if is_valid else "SCANNING"
            
            # Extract Horizon Details
            h5 = multi_horizon.get('h5', 0)
            h15 = multi_horizon.get('h15', 0)
            h30 = multi_horizon.get('h30', 0)

            # Prepare Enhanced Stats
            z_score = current_row.get('volume_zscore', 0)
            adx = current_row.get('adx', 0)
            trend_power = current_row.get('trend_power', 0) 
            
            # Determine Concept/Context based on Regime
            if self.market_regime == "ZOMBIE":
                concept = "Zombie market detected. Stagnant price action. Protection active."
            elif self.market_regime == "RANGING":
                concept = "Mean Reversion active. Hunting overextensions."
            elif self.market_regime == "TRENDING":
                concept = "Trend Following active. Riding momentum."
            elif self.market_regime == "VOLATILE":
                concept = "High Volatility. Defensive stops & wide targets."
            else:
                concept = "Analyzing market structure..."

            oracle_msg = (
                f"\nüîÆ [UNIFIED ORACLE] {self.symbol} | {ready_status}\n"
                f"   Engines Passing: {engines_passing}/3 | Threshold: {self.consensus_threshold}\n"
                f"   Scores  -> ML: {self.engine_scores['ml']:.2f} | SENT: {self.engine_scores['sentiment']:.2f} | TECH: {self.engine_scores['technical']:.2f}\n"
                f"   Horizon -> H5: {h5:.2f} | H15: {h15:.2f} | H30: {h30:.2f}\n"
                f"   Verdict -> Direction: {direction} | Final Conf: {final_confidence:.2f} (Gap: {gap:.2f})\n"
                f"   Phase: {self.market_regime} ({self.regime_confidence*100:.1f}%)\n"
                f"   Concept: {concept}\n"
                f"   Stats: ADX={adx:.1f} | ATR%={atr_pct*100:.2f}% | Trend={trend_power:.2f} | Z-Score={z_score:.2f}\n"
                f"   Confidence: {final_confidence*100:.1f}% | Strategy: Adaptive targets enabled."
            )
            logger.info(oracle_msg)


            # 5. ROBUSTNESS FILTERS
            if atr_pct > self.MAX_ATR_PCT * 1.5:
                logger.warning(f"‚õî [FILTER] {self.symbol} Rejected: Extreme Volatility (ATR: {atr_pct:.2%})")
                return
            
            min_vol = 0.2 if Config.BINANCE_USE_TESTNET else self.MIN_VOLUME_RATIO
            if vol_ratio < min_vol:
                logger.warning(f"‚õî [FILTER] {self.symbol} Rejected: Low Volume (Ratio: {vol_ratio:.2f} < {min_vol})")
                return
            
            if not is_valid:
                self.analysis_stats['filtered_conf'] += 1
                return

            # 6. SIGNAL CREATION
            signal_type = SignalType.LONG if direction == 1 else SignalType.SHORT
            tp_target = self.current_tp_target
            sl_target = self.current_sl_target
            
            # Volatility adjustments
            if atr_pct > 0.03: tp_target *= 1.3; sl_target *= 1.3
            elif atr_pct < 0.01: tp_target *= 0.8; sl_target *= 0.8

            # PHASE 9: PPO Metadata Injection
            # We capture model outputs as the "State" for the Ensemble Weight Optimization policy
            model_outputs = [
                self.individual_model_scores.get('rf', 0.0), 
                self.individual_model_scores.get('xgb', 0.0), 
                self.individual_model_scores.get('gb', 0.0)
            ]
            
            ppo_metadata = {
                "features": current_row.to_dict(), # Raw market features
                "model_outputs": model_outputs,
                "action": float(final_confidence),
                "log_prob": 0.0, # Placeholder for deterministic policy
                "weights": [self.base_rf_weight, self.base_xgb_weight, self.base_gb_weight]
            }

            signal = SignalEvent(
                strategy_id=self.strategy_id,
                symbol=self.symbol,
                datetime=datetime.now(timezone.utc),
                signal_type=signal_type,
                strength=final_confidence,
                atr=current_row['atr'],
                tp_pct=tp_target * 100,
                sl_pct=sl_target * 100,
                current_price=current_row['close'],
                metadata=ppo_metadata
            )
            
            # 7. LOGGING & SUBMISSION
            self.performance_history.append(0)
            self.signal_history.append({
                'timestamp': datetime.now(timezone.utc),
                'type': signal_type,
                'confidence': final_confidence,
                'engines': engines_passing,
                'price': current_row['close']
            })
            
            logger.info(f"‚ú® [UNIVERSAL ENSEMBLE] Signal Generated: {signal_type.name} {self.symbol}")
            self.events_queue.put(signal)
            
            if len(self.performance_history) >= 15:
                self._update_model_weights()
            
            self._last_prediction_time = datetime.now(timezone.utc)
            
        except Exception as e:
            logger.error(f"Universal Ensemble Inference error {self.symbol}: {e}", exc_info=True)
    
    def update_recursive_weights(self, trade_outcome):
        """
        PHASE 9: NEURAL-FORTRESS PPO UPDATE
        Uses TradeOutcome to calculate non-linear rewards and update ensemble weights via PPO.
        """
        try:
            # 1. Validate Input
            from core.reward_system import TradeOutcome # Local import to avoid circular dependency
            
            # Legacy fallback if just a float
            if isinstance(trade_outcome, (float, int)):
                return 

            if not isinstance(trade_outcome, TradeOutcome):
                return

            # 2. Validation: Check if we have PPO metadata
            if not trade_outcome.metadata or 'model_outputs' not in trade_outcome.metadata:
                # logger.debug("Skipping PPO update: No metadata in trade outcome.")
                return

            # 3. Calculate Neural Reward
            # Uses Tanh scaling, Drawdown penalty, and Skewness penalty
            reward = self.reward_system.calculate_reward(trade_outcome)
            
            # 4. Extract Experience Tuple
            # State: Model Probabilities [RF, XGB, GB] -> This is what the weights act upon
            state = np.array(trade_outcome.metadata['model_outputs'])
            
            # Next State: Current Model Probabilities (Approximate with current input or same)
            # For Weight Optimization, S' is slightly ambiguous. We use current input if available.
            next_state = self.last_ensemble_input if self.last_ensemble_input is not None else state
            
            action = trade_outcome.metadata.get('action', 0.5)
            log_prob = trade_outcome.metadata.get('log_prob', 0.0)
            done = True 

            # 5. Store in Prioritized Replay Buffer
            # Ensure state is valid shape
            if state.shape[0] == 3:
                self.memory.add(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    log_prob=log_prob
                )
                # logger.debug(f"üß† [MEMORY] Stored Experience: R={reward:.4f} | State={state}")

            # 6. PPO Batch Learning Trigger
            self.steps_since_learn += 1
            if self.steps_since_learn >= self.training_batch_size and len(self.memory) > self.training_batch_size:
                
                # Sample Batch
                experiences, indices, weights = self.memory.sample(self.training_batch_size)
                states, actions, rewards, next_states, dones, log_probs = experiences
                
                # Current Weights as "Policy"
                current_w = np.array([self.base_rf_weight, self.base_xgb_weight, self.base_gb_weight])
                
                # Perform PPO Update
                new_weights, advantages = self.online_learner.update_ppo_batch(
                    weights=current_w,
                    states=states,
                    actions=actions,
                    rewards=rewards,
                    next_states=next_states,
                    old_log_probs=log_probs,
                    dones=dones
                )
                
                # Update PER Priorities (using advantages as proxy for TD error/importance)
                # We use abs(advantage) because high advantage means "surprising" outcome compared to baseline
                self.memory.update_priorities(indices, np.abs(advantages) + 1e-5)
                
                # Normalize and Apply Weights
                new_weights = np.clip(new_weights, 0.05, 0.80) # prevent extinction
                total_w = np.sum(new_weights)
                if total_w > 0:
                    new_weights /= total_w
                    
                self.base_rf_weight = float(new_weights[0])
                self.base_xgb_weight = float(new_weights[1])
                self.base_gb_weight = float(new_weights[2])
                
                logger.info(f"üß† [PPO UPDATE] New Weights: RF={self.base_rf_weight:.2f} | XGB={self.base_xgb_weight:.2f} | GB={self.base_gb_weight:.2f}")
                
                # Reset Counter
                self.steps_since_learn = 0

        except Exception as e:
            logger.error(f"PPO Update Failed: {e}", exc_info=True)

    def _compute_ml_engine_score(self, rf_proba, xgb_proba, gb_proba, direction: int) -> float:
        """
        Motor ML: Weighted average of ensemble models.
        """
        idx = 1 if direction == 1 else 0  # LONG = idx 1, SHORT = idx 0
        
        ml_score = (
            rf_proba[idx] * self.base_rf_weight +
            xgb_proba[idx] * self.base_xgb_weight +
            gb_proba[idx] * self.base_gb_weight
        )
        
        self.engine_scores['ml'] = ml_score
        return ml_score
    
    def _compute_sentiment_engine_score(self, df) -> float:
        """
        Motor Sentiment: VADER/Social momentum analysis.
        """
        try:
            current_row = df.iloc[-1]
            
            # Get sentiment components
            sentiment = current_row.get('sentiment', 0.0)
            sentiment_momentum = current_row.get('sentiment_momentum', 0.0)
            
            # Normalize to 0-1 range
            # Sentiment is typically -1 to 1, so we map to 0.5 as neutral
            sentiment_normalized = (sentiment + 1) / 2
            
            # Momentum adds weight
            if sentiment_momentum > 0:
                sentiment_score = sentiment_normalized * (1 + min(sentiment_momentum, 0.3))
            else:
                sentiment_score = sentiment_normalized * (1 + max(sentiment_momentum, -0.3))
            
            # Clamp to [0, 1]
            sentiment_score = max(0.0, min(1.0, sentiment_score))
            
            self.engine_scores['sentiment'] = sentiment_score
            return sentiment_score
            
        except Exception:
            self.engine_scores['sentiment'] = 0.5  # Neutral if unavailable
            return 0.5
    
    def _compute_technical_engine_score(self, df, direction: int) -> float:
        """
        Motor T√©cnico: RSI + EMA Cross + Bollinger Bands confluence.
        """
        try:
            current_row = df.iloc[-1]
            score = 0.0
            factors = 0
            
            # RSI Component (weight: 35%)
            rsi = current_row.get('rsi_14', 50)
            if direction == 1:  # LONG
                # RSI should be above 40 but not overbought
                if 40 <= rsi <= 70:
                    score += 0.35 * ((70 - abs(rsi - 55)) / 35)
                    factors += 1
            else:  # SHORT
                # RSI should be below 60 but not oversold
                if 30 <= rsi <= 60:
                    score += 0.35 * ((60 - abs(rsi - 45)) / 35)
                    factors += 1
            
            # EMA Cross Component (weight: 35%)
            ema_cross = current_row.get('ema_20_50_cross', 0)
            if (direction == 1 and ema_cross > 0) or (direction == -1 and ema_cross < 0):
                score += 0.35
                factors += 1
            
            # Bollinger Bands Component (weight: 30%)
            if 'bb_pctb' in current_row:
                bb_pct = current_row['bb_pctb']
                if direction == 1:  # LONG - want price near lower band
                    if bb_pct < 0.3:
                        score += 0.30 * (1 - bb_pct / 0.3)
                        factors += 1
                else:  # SHORT - want price near upper band
                    if bb_pct > 0.7:
                        score += 0.30 * ((bb_pct - 0.7) / 0.3)
                        factors += 1
            
            # Normalize score
            technical_score = score if factors > 0 else 0.5
            
            self.engine_scores['technical'] = technical_score
            return technical_score
            
        except Exception:
            self.engine_scores['technical'] = 0.5
            return 0.5
    
    def compute_organic_confluence(self, df, direction: int,
                                   rf_proba, xgb_proba, gb_proba) -> tuple:
        """
        üéØ ORGANIC CONFLUENCE CALCULATOR
        Returns: (final_confidence, engines_passing, is_valid, multi_horizon)
        """
        # 1. Calculate each engine score
        ml_score = self._compute_ml_engine_score(rf_proba, xgb_proba, gb_proba, direction)
        sentiment_score = self._compute_sentiment_engine_score(df)
        technical_score = self._compute_technical_engine_score(df, direction)
        
        # 2. Dynamic Threshold Logic (World Awareness)
        # PROFESSOR METHOD: adaptamos el rigor seg√∫n la liquidez global.
        ls = getattr(self, 'market_context', {}).get('liquidity_score', 0.8)
        
        # Base is the user's setting (0.75)
        # If LS is low (dead zone), we demand extreme confluence (0.82)
        base_t = self.ENSEMBLE_CONSENSUS_THRESHOLD
        
        if ls >= 0.85:   # PRIME (London/NY)
            dynamic_threshold = base_t
        elif ls >= 0.65: # MID (Tokyo)
            dynamic_threshold = max(base_t, 0.78)
        elif ls >= 0.50: # LOW (Sydney)
            dynamic_threshold = max(base_t, 0.80)
        else:            # DEAD ZONE (22:00-00:00 UTC)
            dynamic_threshold = max(base_t, 0.82)
            
        self.consensus_threshold = dynamic_threshold # Update for oracle logging
        
        # ============================================================
        # ‚è≥ SYNTHETIC MULTI-HORIZON LOGIC (H5, H15, H30)
        # ============================================================
        # H5 (Short): Momentum-heavy (Tech + Sent)
        h5_score = (technical_score * 0.6) + (sentiment_score * 0.4)
        
        # H15 (Mid): Pure consensus
        h15_score = (ml_score * 0.34) + (technical_score * 0.33) + (sentiment_score * 0.33)
        
        # H30 (Full): ML Dominant (Original Model Horizon)
        h30_score = (ml_score * 0.7) + (((technical_score + sentiment_score) / 2) * 0.3)
        
        multi_horizon = {
            'h5': max(0.0, min(1.0, h5_score)),
            'h15': max(0.0, min(1.0, h15_score)),
            'h30': max(0.0, min(1.0, h30_score))
        }

        # Count engines passing threshold
        engines_passing = sum(1 for score in [ml_score, sentiment_score, technical_score]
                             if score >= dynamic_threshold)
        
        # Calculate weighted final confidence
        # ML has higher weight as it's the primary engine
        final_confidence = (
            ml_score * 0.50 +           # 50% weight to ML
            sentiment_score * 0.20 +     # 20% weight to sentiment
            technical_score * 0.30       # 30% weight to technical
        )
        
        # Apply penalty if not enough engines agree
        if engines_passing < self.MIN_ENGINES_REQUIRED:
            penalty = 0.8 if engines_passing == 1 else 0.6
            final_confidence *= penalty
        
        self.engines_active = engines_passing
        is_valid = engines_passing >= self.MIN_ENGINES_REQUIRED
        
        # Phase 8: Neural Bridge Publication
        neural_bridge.publish_insight(
            strategy_id="ML_ENSEMBLE",
            symbol=self.symbol,
            insight={
                'confidence': final_confidence,
                'direction': 'LONG' if direction == 1 else 'SHORT',
                'engines_passing': engines_passing,
                'horizons': multi_horizon
            }
        )
        
        return final_confidence, engines_passing, is_valid, multi_horizon

    
    def get_ensemble_status(self) -> dict:
        """Get current ensemble engine status."""
        return {
            'engines': self.engine_scores.copy(),
            'engines_active': self.engines_active,
            'threshold': self.ENSEMBLE_CONSENSUS_THRESHOLD,
            'is_unified': True,
            'mode': 'UNIVERSAL_ENSEMBLE'
        }


# Add method to base class for backwards compatibility
def _add_ensemble_methods_to_base():
    """Inject ensemble methods into base class."""
    
    def compute_organic_confluence(self, df, direction, rf_proba, xgb_proba, gb_proba):
        """Simplified organic confluence for base class."""
        # ML Engine Score
        idx = 1 if direction == 1 else 0
        ml_score = (
            rf_proba[idx] * self.base_rf_weight +
            xgb_proba[idx] * self.base_xgb_weight +
            gb_proba[idx] * self.base_gb_weight
        )
        
        # Technical Engine Score (from confluence_score)
        technical_score = abs(df.iloc[-1].get('confluence_score', 0))
        technical_score = min(1.0, technical_score + 0.5)  # Normalize
        
        # Count passing engines (ML + Technical, sentiment optional)
        THRESHOLD = 0.60
        engines_passing = sum(1 for s in [ml_score, technical_score] if s >= THRESHOLD)
        
        # Weighted average
        final_conf = ml_score * 0.6 + technical_score * 0.4
        
        if engines_passing < 2:
            final_conf *= 0.8  # Penalty
        
        return final_conf, engines_passing, engines_passing >= 2
    
    # Add method if not exists
    if not hasattr(MLStrategyHybridUltimate, 'compute_organic_confluence'):
        MLStrategyHybridUltimate.compute_organic_confluence = compute_organic_confluence

_add_ensemble_methods_to_base()


# ============================================================
# ‚úÖ FACTORY FUNCTION PARA CREACI√ìN F√ÅCIL
# ============================================================

def create_ml_strategy_hybrid_ultimate(data_provider, events_queue, symbol='BTC/USDT',
                                       sentiment_loader=None, portfolio=None,
                                       initial_capital=12.0, target_capital=100000.0):
    """
    Factory function para crear la estrategia h√≠brida ultimate
    
    Args:
        data_provider: Proveedor de datos
        events_queue: Cola de eventos
        symbol: Par de trading
        sentiment_loader: Cargador de sentiment (opcional)
        portfolio: Portfolio manager (opcional)
        initial_capital: Capital inicial (default: 12 USD)
        target_capital: Capital objetivo (default: 100,000 USD)
    
    Returns:
        UniversalEnsembleStrategy instance (3-engine consensus for ALL symbols)
    """
    # UNIVERSAL ENSEMBLE: All symbols use the same unified strategy
    strategy = UniversalEnsembleStrategy(
        data_provider=data_provider,
        events_queue=events_queue,
        symbol=symbol,
        sentiment_loader=sentiment_loader,
        portfolio=portfolio
    )
    
    # Sobreescribir objetivos si se especifican
    if initial_capital > 0:
        strategy.initial_capital = initial_capital
        strategy.current_capital = initial_capital
    
    if target_capital > 0:
        strategy.target_capital = target_capital
    
    logger.info(
        f"üü¢ UNIVERSAL ENSEMBLE STRATEGY created for {symbol} | "
        f"Goal: ${strategy.initial_capital} ‚Üí ${strategy.target_capital} | "
        f"Engines: ML+Sentiment+Technical | Threshold: 0.60"
    )
    
    return strategy

if __name__ == "__main__":
    """
    Test y demostraci√≥n de la estrategia
    """
    print("=" * 80)
    print("üöÄ ML STRATEGY HYBRID ULTIMATE - DEMONSTRATION")
    print("=" * 80)
    print("‚úÖ FEATURES INCLUDED:")
    print("   1. Ensemble completo: RF + XGB + GB con weighted voting din√°mico")
    print("   2. Detecci√≥n de r√©gimen avanzada con 4 reg√≠menes")
    print("   3. Circuit breaker autom√°tico por drawdown (12%)")
    print("   4. Feature engineering adaptativo (80+ features)")
    print("   5. Targets din√°micos por volatilidad y r√©gimen")
    print("   6. Re-pesado din√°mico basado en performance")
    print("   7. Learning rate adaptativo y factor de agresividad")
    print("   8. Monitoreo completo con 40+ m√©tricas")
    print("   9. Arquitectura as√≠ncrona optimizada")
    print("  10. Gesti√≥n de riesgo multi-capa")
    print("=" * 80)
    print("üéØ OBJETIVO: Convertir $12 USD en $100,000 USD en el menor tiempo posible")
    print("=" * 80)