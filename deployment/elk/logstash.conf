# =============================================================================
# LOGSTASH PIPELINE: TRADER GEMINI FORENSIC INDEXER
# =============================================================================
# QUÉ:    Pipeline Logstash para indexar logs JSON en Elasticsearch.
# POR QUÉ: Permite búsqueda full-text y análisis forense masivo sobre
#           meses de datos de trading que Loki no puede manejar eficientemente.
# PARA QUÉ: Deep-dive en patrones de errores, correlación de señales
#            con ejecución y auditoría de la Trinidad Evolutiva.
# CÓMO:    Recibe logs JSON vía HTTP (Promtail → Logstash), parsea campos
#           del Metal-Core y Trinidad, y los indexa en ES con mapping rico.
# CUÁNDO:  Solo cuando el perfil cold-storage está activo.
# DÓNDE:   deployment/elk/logstash.conf
# QUIÉN:   Logstash (container omega-logstash)
# =============================================================================

input {
  # Promtail envía logs como JSON via HTTP
  http {
    port => 5044
    codec => json
    additional_codecs => { "application/json" => "json" }
  }
}

filter {
  # ─── PASO 1: Parsear JSON del log del bot ───
  if [message] {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }
  }

  # ─── PASO 2: Extraer campos del Metal-Core ───
  mutate {
    rename => {
      "[parsed][level]"     => "log_level"
      "[parsed][module]"    => "module"
      "[parsed][function]"  => "function"
      "[parsed][message]"   => "log_message"
      "[parsed][timestamp]" => "log_timestamp"
      "[parsed][exception]" => "exception"
    }
  }

  # ─── PASO 3: Extraer símbolo del mensaje ───
  grok {
    match => { "log_message" => "(?<symbol>[A-Z]+/USDT)" }
    tag_on_failure => ["_no_symbol"]
  }

  # ─── PASO 4: Clasificar por subsistema ───
  if [module] in ["Engine", "EventLoop", "BoundedQueue"] {
    mutate { add_field => { "subsystem" => "metal_core" } }
  } else if [module] in ["ShadowDarwin", "EvolutionEngine", "Genotype"] {
    mutate { add_field => { "subsystem" => "trinidad_evolutiva" } }
  } else if [module] in ["RiskManager", "KillSwitch", "RiskShield"] {
    mutate { add_field => { "subsystem" => "risk_management" } }
  } else if [module] in ["BinanceExecutor", "OrderManager"] {
    mutate { add_field => { "subsystem" => "execution" } }
  } else if [module] in ["TechnicalStrategy", "MLStrategy", "FusedStrategy"] {
    mutate { add_field => { "subsystem" => "strategies" } }
  } else if [module] in ["DataProvider", "BinanceLoader"] {
    mutate { add_field => { "subsystem" => "data_pipeline" } }
  } else {
    mutate { add_field => { "subsystem" => "other" } }
  }

  # ─── PASO 5: Timestamp override ───
  if [log_timestamp] {
    date {
      match => [ "log_timestamp", "yyyy-MM-dd HH:mm:ss", "ISO8601" ]
      target => "@timestamp"
    }
  }

  # ─── PASO 6: Limpiar campos temporales ───
  mutate {
    remove_field => [ "parsed", "headers", "host" ]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "trader-gemini-%{+YYYY.MM.dd}"
    template_name => "trader-gemini"
    template_overwrite => true
  }
}
